{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch count 0: Loss: 2070.170654296875\n",
      "Epoch count 1: Loss: 1812.0076904296875\n",
      "Epoch count 2: Loss: 1965.1500244140625\n",
      "Epoch count 3: Loss: 2067.736328125\n",
      "Epoch count 4: Loss: 1897.821533203125\n",
      "Epoch count 5: Loss: 1484.716064453125\n",
      "Epoch count 6: Loss: 1355.83056640625\n",
      "Epoch count 7: Loss: 1456.6849365234375\n",
      "Epoch count 8: Loss: 1446.198486328125\n",
      "Epoch count 9: Loss: 1382.376953125\n",
      "Epoch count 10: Loss: 1125.7061767578125\n",
      "Epoch count 11: Loss: 1438.1036376953125\n",
      "Epoch count 12: Loss: 1140.7099609375\n",
      "Epoch count 13: Loss: 1236.64306640625\n",
      "Epoch count 14: Loss: 1406.0333251953125\n",
      "Epoch count 15: Loss: 1114.1751708984375\n",
      "Epoch count 16: Loss: 1035.7381591796875\n",
      "Epoch count 17: Loss: 927.2775268554688\n",
      "Epoch count 18: Loss: 991.0014038085938\n",
      "Epoch count 19: Loss: 814.9093627929688\n",
      "Epoch count 20: Loss: 985.4908447265625\n",
      "Epoch count 21: Loss: 1012.75341796875\n",
      "Epoch count 22: Loss: 831.1987915039062\n",
      "Epoch count 23: Loss: 732.0519409179688\n",
      "Epoch count 24: Loss: 821.6946411132812\n",
      "Epoch count 25: Loss: 891.6630859375\n",
      "Epoch count 26: Loss: 814.9474487304688\n",
      "Epoch count 27: Loss: 739.8709716796875\n",
      "Epoch count 28: Loss: 792.7413330078125\n",
      "Epoch count 29: Loss: 713.1015625\n",
      "Epoch count 30: Loss: 752.9699096679688\n",
      "Epoch count 31: Loss: 741.124267578125\n",
      "Epoch count 32: Loss: 652.4308471679688\n",
      "Epoch count 33: Loss: 691.1260986328125\n",
      "Epoch count 34: Loss: 694.0064086914062\n",
      "Epoch count 35: Loss: 656.517822265625\n",
      "Epoch count 36: Loss: 586.84130859375\n",
      "Epoch count 37: Loss: 657.3572998046875\n",
      "Epoch count 38: Loss: 590.8953857421875\n",
      "Epoch count 39: Loss: 626.08154296875\n",
      "Epoch count 40: Loss: 669.489990234375\n",
      "Epoch count 41: Loss: 564.3325805664062\n",
      "Epoch count 42: Loss: 622.61669921875\n",
      "Epoch count 43: Loss: 558.7052612304688\n",
      "Epoch count 44: Loss: 576.8159790039062\n",
      "Epoch count 45: Loss: 606.0719604492188\n",
      "Epoch count 46: Loss: 550.890625\n",
      "Epoch count 47: Loss: 604.90283203125\n",
      "Epoch count 48: Loss: 541.5123291015625\n",
      "Epoch count 49: Loss: 549.2654418945312\n",
      "Epoch count 50: Loss: 555.245361328125\n",
      "Epoch count 51: Loss: 524.9771728515625\n",
      "Epoch count 52: Loss: 526.2079467773438\n",
      "Epoch count 53: Loss: 522.9950561523438\n",
      "Epoch count 54: Loss: 522.7438354492188\n",
      "Epoch count 55: Loss: 574.823486328125\n",
      "Epoch count 56: Loss: 550.2086791992188\n",
      "Epoch count 57: Loss: 542.745849609375\n",
      "Epoch count 58: Loss: 520.5247802734375\n",
      "Epoch count 59: Loss: 533.8851928710938\n",
      "Epoch count 60: Loss: 547.2845458984375\n",
      "Epoch count 61: Loss: 526.5902099609375\n",
      "Epoch count 62: Loss: 543.4563598632812\n",
      "Epoch count 63: Loss: 513.882568359375\n",
      "Epoch count 64: Loss: 550.5018920898438\n",
      "Epoch count 65: Loss: 505.9161071777344\n",
      "Epoch count 66: Loss: 488.2179260253906\n",
      "Epoch count 67: Loss: 509.6098937988281\n",
      "Epoch count 68: Loss: 508.4532775878906\n",
      "Epoch count 69: Loss: 509.8404235839844\n",
      "Epoch count 70: Loss: 502.355224609375\n",
      "Epoch count 71: Loss: 512.3087158203125\n",
      "Epoch count 72: Loss: 518.5751342773438\n",
      "Epoch count 73: Loss: 493.4573669433594\n",
      "Epoch count 74: Loss: 513.2703857421875\n",
      "Epoch count 75: Loss: 482.9423828125\n",
      "Epoch count 76: Loss: 503.7060241699219\n",
      "Epoch count 77: Loss: 486.7891845703125\n",
      "Epoch count 78: Loss: 504.3614196777344\n",
      "Epoch count 79: Loss: 503.59429931640625\n",
      "Epoch count 80: Loss: 503.122314453125\n",
      "Epoch count 81: Loss: 501.3408508300781\n",
      "Epoch count 82: Loss: 492.52880859375\n",
      "Epoch count 83: Loss: 493.2904052734375\n",
      "Epoch count 84: Loss: 509.0146484375\n",
      "Epoch count 85: Loss: 494.2886962890625\n",
      "Epoch count 86: Loss: 494.2034912109375\n",
      "Epoch count 87: Loss: 492.74908447265625\n",
      "Epoch count 88: Loss: 497.2655334472656\n",
      "Epoch count 89: Loss: 496.188720703125\n",
      "Epoch count 90: Loss: 483.7687683105469\n",
      "Epoch count 91: Loss: 491.49188232421875\n",
      "Epoch count 92: Loss: 498.8877258300781\n",
      "Epoch count 93: Loss: 495.5843505859375\n",
      "Epoch count 94: Loss: 490.63665771484375\n",
      "Epoch count 95: Loss: 501.616455078125\n",
      "Epoch count 96: Loss: 490.7679748535156\n",
      "Epoch count 97: Loss: 485.6843566894531\n",
      "Epoch count 98: Loss: 494.1700134277344\n",
      "Epoch count 99: Loss: 497.78961181640625\n",
      "Epoch count 100: Loss: 494.4059753417969\n",
      "Epoch count 101: Loss: 491.5010070800781\n",
      "Epoch count 102: Loss: 494.8009338378906\n",
      "Epoch count 103: Loss: 492.0061340332031\n",
      "Epoch count 104: Loss: 501.7168273925781\n",
      "Epoch count 105: Loss: 497.6397705078125\n",
      "Epoch count 106: Loss: 488.28240966796875\n",
      "Epoch count 107: Loss: 494.9715270996094\n",
      "Epoch count 108: Loss: 489.6070251464844\n",
      "Epoch count 109: Loss: 495.26287841796875\n",
      "Epoch count 110: Loss: 490.45928955078125\n",
      "Epoch count 111: Loss: 491.7737731933594\n",
      "Epoch count 112: Loss: 495.1550598144531\n",
      "Epoch count 113: Loss: 494.96197509765625\n",
      "Epoch count 114: Loss: 494.3341369628906\n",
      "Epoch count 115: Loss: 493.5085754394531\n",
      "Epoch count 116: Loss: 492.55706787109375\n",
      "Epoch count 117: Loss: 493.3586120605469\n",
      "Epoch count 118: Loss: 494.13446044921875\n",
      "Epoch count 119: Loss: 492.36322021484375\n",
      "Epoch count 120: Loss: 493.45074462890625\n",
      "Epoch count 121: Loss: 493.4862365722656\n",
      "Epoch count 122: Loss: 491.9393615722656\n",
      "Epoch count 123: Loss: 494.064697265625\n",
      "Epoch count 124: Loss: 492.91259765625\n",
      "Epoch count 125: Loss: 492.4353942871094\n",
      "Epoch count 126: Loss: 491.0476379394531\n",
      "Epoch count 127: Loss: 492.5519714355469\n",
      "Epoch count 128: Loss: 490.4223937988281\n",
      "Epoch count 129: Loss: 490.41522216796875\n",
      "Epoch count 130: Loss: 491.1903381347656\n",
      "Epoch count 131: Loss: 492.9978942871094\n",
      "Epoch count 132: Loss: 492.4712219238281\n",
      "Epoch count 133: Loss: 491.70849609375\n",
      "Epoch count 134: Loss: 490.74090576171875\n",
      "Epoch count 135: Loss: 491.90838623046875\n",
      "Epoch count 136: Loss: 491.80657958984375\n",
      "Epoch count 137: Loss: 489.1050109863281\n",
      "Epoch count 138: Loss: 490.7594299316406\n",
      "Epoch count 139: Loss: 492.302734375\n",
      "Epoch count 140: Loss: 492.12091064453125\n",
      "Epoch count 141: Loss: 492.07672119140625\n",
      "Epoch count 142: Loss: 492.64459228515625\n",
      "Epoch count 143: Loss: 490.9906311035156\n",
      "Epoch count 144: Loss: 490.2206726074219\n",
      "Epoch count 145: Loss: 490.5168762207031\n",
      "Epoch count 146: Loss: 492.5543518066406\n",
      "Epoch count 147: Loss: 491.87884521484375\n",
      "Epoch count 148: Loss: 492.0758056640625\n",
      "Epoch count 149: Loss: 489.494384765625\n",
      "Epoch count 150: Loss: 491.1811218261719\n",
      "Epoch count 151: Loss: 493.5137634277344\n",
      "Epoch count 152: Loss: 491.1475830078125\n",
      "Epoch count 153: Loss: 491.7667236328125\n",
      "Epoch count 154: Loss: 491.89556884765625\n",
      "Epoch count 155: Loss: 490.9943542480469\n",
      "Epoch count 156: Loss: 490.63470458984375\n",
      "Epoch count 157: Loss: 490.6999206542969\n",
      "Epoch count 158: Loss: 491.4062194824219\n",
      "Epoch count 159: Loss: 491.3141784667969\n",
      "Epoch count 160: Loss: 491.64166259765625\n",
      "Epoch count 161: Loss: 490.32928466796875\n",
      "Epoch count 162: Loss: 490.813720703125\n",
      "Epoch count 163: Loss: 492.5602722167969\n",
      "Epoch count 164: Loss: 491.4092712402344\n",
      "Epoch count 165: Loss: 490.6888732910156\n",
      "Epoch count 166: Loss: 490.4893493652344\n",
      "Epoch count 167: Loss: 490.9842834472656\n",
      "Epoch count 168: Loss: 491.5667419433594\n",
      "Epoch count 169: Loss: 490.95074462890625\n",
      "Epoch count 170: Loss: 489.69329833984375\n",
      "Epoch count 171: Loss: 490.8030700683594\n",
      "Epoch count 172: Loss: 491.1287536621094\n",
      "Epoch count 173: Loss: 491.340087890625\n",
      "Epoch count 174: Loss: 490.64990234375\n",
      "Epoch count 175: Loss: 489.7584228515625\n",
      "Epoch count 176: Loss: 492.10302734375\n",
      "Epoch count 177: Loss: 491.8385009765625\n",
      "Epoch count 178: Loss: 491.2660217285156\n",
      "Epoch count 179: Loss: 490.98284912109375\n",
      "Epoch count 180: Loss: 491.77117919921875\n",
      "Epoch count 181: Loss: 490.5812683105469\n",
      "Epoch count 182: Loss: 490.7524719238281\n",
      "Epoch count 183: Loss: 490.61517333984375\n",
      "Epoch count 184: Loss: 490.79583740234375\n",
      "Epoch count 185: Loss: 490.954833984375\n",
      "Epoch count 186: Loss: 490.2088623046875\n",
      "Epoch count 187: Loss: 490.109375\n",
      "Epoch count 188: Loss: 490.49951171875\n",
      "Epoch count 189: Loss: 490.1559753417969\n",
      "Epoch count 190: Loss: 490.0408630371094\n",
      "Epoch count 191: Loss: 491.6986389160156\n",
      "Epoch count 192: Loss: 490.3255920410156\n",
      "Epoch count 193: Loss: 490.1813049316406\n",
      "Epoch count 194: Loss: 489.2590637207031\n",
      "Epoch count 195: Loss: 490.5219421386719\n",
      "Epoch count 196: Loss: 490.5841979980469\n",
      "Epoch count 197: Loss: 488.4625244140625\n",
      "Epoch count 198: Loss: 489.44384765625\n",
      "Epoch count 199: Loss: 492.62469482421875\n",
      "Epoch count 200: Loss: 492.2181701660156\n",
      "Epoch count 201: Loss: 491.1623840332031\n",
      "Epoch count 202: Loss: 490.1331787109375\n",
      "Epoch count 203: Loss: 490.982421875\n",
      "Epoch count 204: Loss: 489.8465881347656\n",
      "Epoch count 205: Loss: 491.1073913574219\n",
      "Epoch count 206: Loss: 490.2576904296875\n",
      "Epoch count 207: Loss: 489.9681396484375\n",
      "Epoch count 208: Loss: 490.16387939453125\n",
      "Epoch count 209: Loss: 490.7289733886719\n",
      "Epoch count 210: Loss: 490.1009521484375\n",
      "Epoch count 211: Loss: 489.9090270996094\n",
      "Epoch count 212: Loss: 490.2179870605469\n",
      "Epoch count 213: Loss: 489.9338684082031\n",
      "Epoch count 214: Loss: 489.7915954589844\n",
      "Epoch count 215: Loss: 489.46356201171875\n",
      "Epoch count 216: Loss: 490.7209167480469\n",
      "Epoch count 217: Loss: 490.0711669921875\n",
      "Epoch count 218: Loss: 489.9715270996094\n",
      "Epoch count 219: Loss: 489.906494140625\n",
      "Epoch count 220: Loss: 489.7820739746094\n",
      "Epoch count 221: Loss: 489.8487548828125\n",
      "Epoch count 222: Loss: 490.0223083496094\n",
      "Epoch count 223: Loss: 489.9607849121094\n",
      "Epoch count 224: Loss: 490.3005065917969\n",
      "Epoch count 225: Loss: 489.88037109375\n",
      "Epoch count 226: Loss: 489.6040954589844\n",
      "Epoch count 227: Loss: 489.42156982421875\n",
      "Epoch count 228: Loss: 489.93231201171875\n",
      "Epoch count 229: Loss: 490.7530517578125\n",
      "Epoch count 230: Loss: 489.4953918457031\n",
      "Epoch count 231: Loss: 489.40472412109375\n",
      "Epoch count 232: Loss: 488.728759765625\n",
      "Epoch count 233: Loss: 489.0810852050781\n",
      "Epoch count 234: Loss: 490.4673156738281\n",
      "Epoch count 235: Loss: 488.7955017089844\n",
      "Epoch count 236: Loss: 489.20867919921875\n",
      "Epoch count 237: Loss: 489.9049377441406\n",
      "Epoch count 238: Loss: 491.4219970703125\n",
      "Epoch count 239: Loss: 489.71356201171875\n",
      "Epoch count 240: Loss: 489.0205383300781\n",
      "Epoch count 241: Loss: 489.435302734375\n",
      "Epoch count 242: Loss: 488.6171569824219\n",
      "Epoch count 243: Loss: 490.29681396484375\n",
      "Epoch count 244: Loss: 488.2386779785156\n",
      "Epoch count 245: Loss: 488.40069580078125\n",
      "Epoch count 246: Loss: 489.5733337402344\n",
      "Epoch count 247: Loss: 490.1805114746094\n",
      "Epoch count 248: Loss: 491.6842956542969\n",
      "Epoch count 249: Loss: 488.7771911621094\n",
      "Epoch count 250: Loss: 488.5506896972656\n",
      "Epoch count 251: Loss: 489.6484680175781\n",
      "Epoch count 252: Loss: 488.88641357421875\n",
      "Epoch count 253: Loss: 490.89678955078125\n",
      "Epoch count 254: Loss: 489.1627502441406\n",
      "Epoch count 255: Loss: 489.33837890625\n",
      "Epoch count 256: Loss: 489.25927734375\n",
      "Epoch count 257: Loss: 490.404296875\n",
      "Epoch count 258: Loss: 488.91656494140625\n",
      "Epoch count 259: Loss: 488.45574951171875\n",
      "Epoch count 260: Loss: 489.26861572265625\n",
      "Epoch count 261: Loss: 490.14227294921875\n",
      "Epoch count 262: Loss: 489.0791931152344\n",
      "Epoch count 263: Loss: 490.0635070800781\n",
      "Epoch count 264: Loss: 488.88128662109375\n",
      "Epoch count 265: Loss: 489.41937255859375\n",
      "Epoch count 266: Loss: 488.63031005859375\n",
      "Epoch count 267: Loss: 489.5085144042969\n",
      "Epoch count 268: Loss: 489.2397766113281\n",
      "Epoch count 269: Loss: 488.97454833984375\n",
      "Epoch count 270: Loss: 488.9664001464844\n",
      "Epoch count 271: Loss: 488.9271545410156\n",
      "Epoch count 272: Loss: 488.8477478027344\n",
      "Epoch count 273: Loss: 488.9064636230469\n",
      "Epoch count 274: Loss: 489.2503967285156\n",
      "Epoch count 275: Loss: 488.29150390625\n",
      "Epoch count 276: Loss: 488.7823486328125\n",
      "Epoch count 277: Loss: 488.4375305175781\n",
      "Epoch count 278: Loss: 487.8930969238281\n",
      "Epoch count 279: Loss: 489.98193359375\n",
      "Epoch count 280: Loss: 489.53021240234375\n",
      "Epoch count 281: Loss: 489.1485290527344\n",
      "Epoch count 282: Loss: 488.64703369140625\n",
      "Epoch count 283: Loss: 488.620361328125\n",
      "Epoch count 284: Loss: 489.2053527832031\n",
      "Epoch count 285: Loss: 488.5909423828125\n",
      "Epoch count 286: Loss: 488.29095458984375\n",
      "Epoch count 287: Loss: 489.0571594238281\n",
      "Epoch count 288: Loss: 488.6548767089844\n",
      "Epoch count 289: Loss: 488.84814453125\n",
      "Epoch count 290: Loss: 488.5088806152344\n",
      "Epoch count 291: Loss: 488.75836181640625\n",
      "Epoch count 292: Loss: 488.3175354003906\n",
      "Epoch count 293: Loss: 488.4881286621094\n",
      "Epoch count 294: Loss: 488.3603820800781\n",
      "Epoch count 295: Loss: 488.11358642578125\n",
      "Epoch count 296: Loss: 487.6927490234375\n",
      "Epoch count 297: Loss: 488.26129150390625\n",
      "Epoch count 298: Loss: 487.8418884277344\n",
      "Epoch count 299: Loss: 485.7995910644531\n",
      "Epoch count 300: Loss: 488.792236328125\n",
      "Epoch count 301: Loss: 487.7242431640625\n",
      "Epoch count 302: Loss: 489.69488525390625\n",
      "Epoch count 303: Loss: 487.4333190917969\n",
      "Epoch count 304: Loss: 486.1124267578125\n",
      "Epoch count 305: Loss: 487.58233642578125\n",
      "Epoch count 306: Loss: 490.2688293457031\n",
      "Epoch count 307: Loss: 487.96453857421875\n",
      "Epoch count 308: Loss: 487.76397705078125\n",
      "Epoch count 309: Loss: 489.02191162109375\n",
      "Epoch count 310: Loss: 488.7741394042969\n",
      "Epoch count 311: Loss: 490.2896423339844\n",
      "Epoch count 312: Loss: 488.8046875\n",
      "Epoch count 313: Loss: 487.5693664550781\n",
      "Epoch count 314: Loss: 489.18902587890625\n",
      "Epoch count 315: Loss: 486.8240051269531\n",
      "Epoch count 316: Loss: 486.6514892578125\n",
      "Epoch count 317: Loss: 489.37890625\n",
      "Epoch count 318: Loss: 487.53753662109375\n",
      "Epoch count 319: Loss: 485.8228454589844\n",
      "Epoch count 320: Loss: 486.44049072265625\n",
      "Epoch count 321: Loss: 487.6067199707031\n",
      "Epoch count 322: Loss: 490.4648742675781\n",
      "Epoch count 323: Loss: 487.0803527832031\n",
      "Epoch count 324: Loss: 489.28558349609375\n",
      "Epoch count 325: Loss: 488.890869140625\n",
      "Epoch count 326: Loss: 487.37701416015625\n",
      "Epoch count 327: Loss: 487.0385437011719\n",
      "Epoch count 328: Loss: 486.7918701171875\n",
      "Epoch count 329: Loss: 486.6127014160156\n",
      "Epoch count 330: Loss: 487.9827575683594\n",
      "Epoch count 331: Loss: 489.9176025390625\n",
      "Epoch count 332: Loss: 486.3624267578125\n",
      "Epoch count 333: Loss: 488.4079284667969\n",
      "Epoch count 334: Loss: 488.793701171875\n",
      "Epoch count 335: Loss: 487.2239074707031\n",
      "Epoch count 336: Loss: 487.4404296875\n",
      "Epoch count 337: Loss: 487.87432861328125\n",
      "Epoch count 338: Loss: 488.51690673828125\n",
      "Epoch count 339: Loss: 485.6725769042969\n",
      "Epoch count 340: Loss: 488.1021423339844\n",
      "Epoch count 341: Loss: 490.5028381347656\n",
      "Epoch count 342: Loss: 487.7662658691406\n",
      "Epoch count 343: Loss: 487.5087585449219\n",
      "Epoch count 344: Loss: 488.34246826171875\n",
      "Epoch count 345: Loss: 487.37677001953125\n",
      "Epoch count 346: Loss: 487.8694152832031\n",
      "Epoch count 347: Loss: 487.195556640625\n",
      "Epoch count 348: Loss: 487.8832092285156\n",
      "Epoch count 349: Loss: 487.2804870605469\n",
      "Epoch count 350: Loss: 487.2920227050781\n",
      "Epoch count 351: Loss: 487.25225830078125\n",
      "Epoch count 352: Loss: 487.8340759277344\n",
      "Epoch count 353: Loss: 487.0931396484375\n",
      "Epoch count 354: Loss: 487.7447814941406\n",
      "Epoch count 355: Loss: 487.36688232421875\n",
      "Epoch count 356: Loss: 487.9660339355469\n",
      "Epoch count 357: Loss: 487.05609130859375\n",
      "Epoch count 358: Loss: 487.6443786621094\n",
      "Epoch count 359: Loss: 487.3819580078125\n",
      "Epoch count 360: Loss: 487.4471435546875\n",
      "Epoch count 361: Loss: 487.34759521484375\n",
      "Epoch count 362: Loss: 487.2022705078125\n",
      "Epoch count 363: Loss: 487.0899658203125\n",
      "Epoch count 364: Loss: 487.16571044921875\n",
      "Epoch count 365: Loss: 486.5710754394531\n",
      "Epoch count 366: Loss: 487.04656982421875\n",
      "Epoch count 367: Loss: 487.080322265625\n",
      "Epoch count 368: Loss: 486.9276428222656\n",
      "Epoch count 369: Loss: 486.8331298828125\n",
      "Epoch count 370: Loss: 488.51104736328125\n",
      "Epoch count 371: Loss: 487.1411437988281\n",
      "Epoch count 372: Loss: 487.1622009277344\n",
      "Epoch count 373: Loss: 486.3475036621094\n",
      "Epoch count 374: Loss: 487.019775390625\n",
      "Epoch count 375: Loss: 487.5386047363281\n",
      "Epoch count 376: Loss: 486.4466552734375\n",
      "Epoch count 377: Loss: 485.6429443359375\n",
      "Epoch count 378: Loss: 487.29595947265625\n",
      "Epoch count 379: Loss: 485.154296875\n",
      "Epoch count 380: Loss: 488.16510009765625\n",
      "Epoch count 381: Loss: 487.392822265625\n",
      "Epoch count 382: Loss: 488.1648864746094\n",
      "Epoch count 383: Loss: 486.4428405761719\n",
      "Epoch count 384: Loss: 485.6724853515625\n",
      "Epoch count 385: Loss: 487.6087646484375\n",
      "Epoch count 386: Loss: 487.72576904296875\n",
      "Epoch count 387: Loss: 485.6736145019531\n",
      "Epoch count 388: Loss: 486.8255310058594\n",
      "Epoch count 389: Loss: 487.2015075683594\n",
      "Epoch count 390: Loss: 486.7127990722656\n",
      "Epoch count 391: Loss: 486.7317199707031\n",
      "Epoch count 392: Loss: 486.08331298828125\n",
      "Epoch count 393: Loss: 487.17596435546875\n",
      "Epoch count 394: Loss: 486.5403747558594\n",
      "Epoch count 395: Loss: 485.5907897949219\n",
      "Epoch count 396: Loss: 487.052734375\n",
      "Epoch count 397: Loss: 486.1436462402344\n",
      "Epoch count 398: Loss: 486.4510803222656\n",
      "Epoch count 399: Loss: 486.5905456542969\n",
      "Epoch count 400: Loss: 486.49530029296875\n",
      "Epoch count 401: Loss: 485.9319763183594\n",
      "Epoch count 402: Loss: 485.9153137207031\n",
      "Epoch count 403: Loss: 486.5520324707031\n",
      "Epoch count 404: Loss: 485.8294677734375\n",
      "Epoch count 405: Loss: 486.04412841796875\n",
      "Epoch count 406: Loss: 485.3655090332031\n",
      "Epoch count 407: Loss: 486.573486328125\n",
      "Epoch count 408: Loss: 487.7994384765625\n",
      "Epoch count 409: Loss: 487.06329345703125\n",
      "Epoch count 410: Loss: 486.0082702636719\n",
      "Epoch count 411: Loss: 484.7080078125\n",
      "Epoch count 412: Loss: 486.59527587890625\n",
      "Epoch count 413: Loss: 486.9507141113281\n",
      "Epoch count 414: Loss: 486.68243408203125\n",
      "Epoch count 415: Loss: 485.70306396484375\n",
      "Epoch count 416: Loss: 485.6521911621094\n",
      "Epoch count 417: Loss: 486.50360107421875\n",
      "Epoch count 418: Loss: 485.6016540527344\n",
      "Epoch count 419: Loss: 485.9468078613281\n",
      "Epoch count 420: Loss: 486.7329406738281\n",
      "Epoch count 421: Loss: 485.6289978027344\n",
      "Epoch count 422: Loss: 486.0991516113281\n",
      "Epoch count 423: Loss: 486.1754455566406\n",
      "Epoch count 424: Loss: 486.0514221191406\n",
      "Epoch count 425: Loss: 485.47735595703125\n",
      "Epoch count 426: Loss: 486.3659362792969\n",
      "Epoch count 427: Loss: 486.5709228515625\n",
      "Epoch count 428: Loss: 485.2945251464844\n",
      "Epoch count 429: Loss: 485.0602111816406\n",
      "Epoch count 430: Loss: 484.79345703125\n",
      "Epoch count 431: Loss: 486.3704528808594\n",
      "Epoch count 432: Loss: 484.9206237792969\n",
      "Epoch count 433: Loss: 484.8592224121094\n",
      "Epoch count 434: Loss: 485.9626770019531\n",
      "Epoch count 435: Loss: 485.76824951171875\n",
      "Epoch count 436: Loss: 485.0779113769531\n",
      "Epoch count 437: Loss: 487.2006530761719\n",
      "Epoch count 438: Loss: 484.3283996582031\n",
      "Epoch count 439: Loss: 485.70379638671875\n",
      "Epoch count 440: Loss: 485.75567626953125\n",
      "Epoch count 441: Loss: 484.7008361816406\n",
      "Epoch count 442: Loss: 485.3066101074219\n",
      "Epoch count 443: Loss: 485.5907287597656\n",
      "Epoch count 444: Loss: 485.2058410644531\n",
      "Epoch count 445: Loss: 484.9498596191406\n",
      "Epoch count 446: Loss: 485.3285827636719\n",
      "Epoch count 447: Loss: 485.4071350097656\n",
      "Epoch count 448: Loss: 485.4677734375\n",
      "Epoch count 449: Loss: 486.3360290527344\n",
      "Epoch count 450: Loss: 487.6236877441406\n",
      "Epoch count 451: Loss: 485.5736083984375\n",
      "Epoch count 452: Loss: 484.9823303222656\n",
      "Epoch count 453: Loss: 487.0715026855469\n",
      "Epoch count 454: Loss: 486.5174865722656\n",
      "Epoch count 455: Loss: 485.6676940917969\n",
      "Epoch count 456: Loss: 484.8544616699219\n",
      "Epoch count 457: Loss: 484.5341796875\n",
      "Epoch count 458: Loss: 485.34832763671875\n",
      "Epoch count 459: Loss: 484.22998046875\n",
      "Epoch count 460: Loss: 485.68157958984375\n",
      "Epoch count 461: Loss: 485.0275573730469\n",
      "Epoch count 462: Loss: 485.0129089355469\n",
      "Epoch count 463: Loss: 486.6347351074219\n",
      "Epoch count 464: Loss: 485.5027160644531\n",
      "Epoch count 465: Loss: 485.1877746582031\n",
      "Epoch count 466: Loss: 485.6639099121094\n",
      "Epoch count 467: Loss: 484.3968811035156\n",
      "Epoch count 468: Loss: 485.6011047363281\n",
      "Epoch count 469: Loss: 484.2170715332031\n",
      "Epoch count 470: Loss: 484.2979736328125\n",
      "Epoch count 471: Loss: 483.7658386230469\n",
      "Epoch count 472: Loss: 485.0729064941406\n",
      "Epoch count 473: Loss: 485.38690185546875\n",
      "Epoch count 474: Loss: 485.2192077636719\n",
      "Epoch count 475: Loss: 486.67547607421875\n",
      "Epoch count 476: Loss: 484.1156921386719\n",
      "Epoch count 477: Loss: 486.3921203613281\n",
      "Epoch count 478: Loss: 484.71490478515625\n",
      "Epoch count 479: Loss: 485.511962890625\n",
      "Epoch count 480: Loss: 484.6794738769531\n",
      "Epoch count 481: Loss: 485.4176025390625\n",
      "Epoch count 482: Loss: 484.45892333984375\n",
      "Epoch count 483: Loss: 485.0129699707031\n",
      "Epoch count 484: Loss: 484.9168395996094\n",
      "Epoch count 485: Loss: 484.2269592285156\n",
      "Epoch count 486: Loss: 484.1308898925781\n",
      "Epoch count 487: Loss: 484.4961853027344\n",
      "Epoch count 488: Loss: 483.92706298828125\n",
      "Epoch count 489: Loss: 484.47991943359375\n",
      "Epoch count 490: Loss: 484.1990966796875\n",
      "Epoch count 491: Loss: 483.0611267089844\n",
      "Epoch count 492: Loss: 483.1259765625\n",
      "Epoch count 493: Loss: 484.5487976074219\n",
      "Epoch count 494: Loss: 485.7483825683594\n",
      "Epoch count 495: Loss: 484.26409912109375\n",
      "Epoch count 496: Loss: 485.5496826171875\n",
      "Epoch count 497: Loss: 484.787353515625\n",
      "Epoch count 498: Loss: 483.4374694824219\n",
      "Epoch count 499: Loss: 485.0718994140625\n",
      "Weight: [0.9956384 2.0075164], Bias: 1.2210146188735962\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression project with generalized input dimensions and L2 normalisation\n",
    "__author__ = \"Billy Cao\"\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "import generator\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)  # change to false when running on ML server\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "batchSize = 100\n",
    "gen = generator.gen2d(batchSize)  # gen2d for 2D input\n",
    "epochs = 500\n",
    "regTerm = 0.001\n",
    "learning_rate = 1e-5\n",
    "\n",
    "class LinearModel:  # initializing to 1 now, but also can do 0 or random\n",
    "    def __call__(self, x):  # predicting function\n",
    "        return self.Weight * x + self.Bias\n",
    "\n",
    "    def __init__(self):\n",
    "        self.Weight = tf.Variable(1.0, shape=tf.TensorShape(None))  # initialize m to any shape\n",
    "        self.Bias = tf.Variable(1.0)\n",
    "\n",
    "\n",
    "def loss(y, pred):  # Mean Squared Error with L2 Normalisation\n",
    "    return tf.reduce_mean(tf.square(y - pred)) + tf.reduce_sum(regTerm * tf.square(linear_model.Weight))\n",
    "\n",
    "\n",
    "def train(linear_model, x, y, lr):\n",
    "    # use to reshape into 2D vector if input is of unknown dimension\n",
    "    # if len(x.shape) == 1:\n",
    "    #     X = tf.reshape(x, [x.shape[0], 1])\n",
    "    with tf.GradientTape(persistent=False) as t:  # persistent=True is needed if assigning dy_dWeight, dy_dBias in 2 lines. Limits the times u can call it to once\n",
    "        current_loss = loss(y, linear_model(x))\n",
    "    dy_dWeight, dy_dBias = t.gradient(current_loss, [linear_model.Weight, linear_model.Bias])\n",
    "    linear_model.Weight.assign_sub(lr * dy_dWeight)\n",
    "    linear_model.Bias.assign_sub(lr * dy_dBias)\n",
    "\n",
    "\n",
    "linear_model = LinearModel()\n",
    "sampleX, sampleY = next(gen)\n",
    "linear_model.Weight.assign([1.0] * sampleX.shape[-1])  # initialize m to 1.0 and make it same dimension as the input\n",
    "for epoch_count in range(epochs):\n",
    "    x, y = next(gen)\n",
    "    real_loss = loss(y, linear_model(x))\n",
    "    train(linear_model, x, y, lr=learning_rate)\n",
    "    print(f\"Epoch count {epoch_count}: Loss: {real_loss.numpy()}\")\n",
    "\n",
    "# Assuming the weights are correct, the loss will be directly related to error in Bias. Thus we compensate it here\n",
    "linear_model.Bias.assign_sub(linear_model.Bias - sqrt(real_loss.numpy()))\n",
    "print(f'Weight: {linear_model.Weight.numpy()}, Bias: {linear_model.Bias.numpy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}