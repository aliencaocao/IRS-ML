{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch count 0: Loss: 2158.787353515625\n",
      "Epoch count 1: Loss: 2315.848876953125\n",
      "Epoch count 2: Loss: 2198.573486328125\n",
      "Epoch count 3: Loss: 1665.6226806640625\n",
      "Epoch count 4: Loss: 1697.5770263671875\n",
      "Epoch count 5: Loss: 1695.603515625\n",
      "Epoch count 6: Loss: 1368.552001953125\n",
      "Epoch count 7: Loss: 1630.1988525390625\n",
      "Epoch count 8: Loss: 1295.3438720703125\n",
      "Epoch count 9: Loss: 1354.936279296875\n",
      "Epoch count 10: Loss: 1382.005126953125\n",
      "Epoch count 11: Loss: 1460.1453857421875\n",
      "Epoch count 12: Loss: 1138.7392578125\n",
      "Epoch count 13: Loss: 1290.2537841796875\n",
      "Epoch count 14: Loss: 1154.2894287109375\n",
      "Epoch count 15: Loss: 1142.541259765625\n",
      "Epoch count 16: Loss: 1003.1268920898438\n",
      "Epoch count 17: Loss: 899.2947998046875\n",
      "Epoch count 18: Loss: 955.3427124023438\n",
      "Epoch count 19: Loss: 883.1469116210938\n",
      "Epoch count 20: Loss: 1035.14208984375\n",
      "Epoch count 21: Loss: 972.15087890625\n",
      "Epoch count 22: Loss: 726.7020874023438\n",
      "Epoch count 23: Loss: 917.0420532226562\n",
      "Epoch count 24: Loss: 689.1705322265625\n",
      "Epoch count 25: Loss: 756.9495239257812\n",
      "Epoch count 26: Loss: 820.3209838867188\n",
      "Epoch count 27: Loss: 744.3096313476562\n",
      "Epoch count 28: Loss: 675.9652709960938\n",
      "Epoch count 29: Loss: 656.0252685546875\n",
      "Epoch count 30: Loss: 721.3474731445312\n",
      "Epoch count 31: Loss: 701.0982666015625\n",
      "Epoch count 32: Loss: 650.3057250976562\n",
      "Epoch count 33: Loss: 633.7779541015625\n",
      "Epoch count 34: Loss: 735.9630126953125\n",
      "Epoch count 35: Loss: 669.2547607421875\n",
      "Epoch count 36: Loss: 659.9917602539062\n",
      "Epoch count 37: Loss: 635.0317993164062\n",
      "Epoch count 38: Loss: 653.623291015625\n",
      "Epoch count 39: Loss: 608.3580322265625\n",
      "Epoch count 40: Loss: 584.6320190429688\n",
      "Epoch count 41: Loss: 595.3719482421875\n",
      "Epoch count 42: Loss: 615.2820434570312\n",
      "Epoch count 43: Loss: 605.2178955078125\n",
      "Epoch count 44: Loss: 565.9700927734375\n",
      "Epoch count 45: Loss: 549.7357177734375\n",
      "Epoch count 46: Loss: 589.4119873046875\n",
      "Epoch count 47: Loss: 602.4767456054688\n",
      "Epoch count 48: Loss: 573.4490356445312\n",
      "Epoch count 49: Loss: 550.6400756835938\n",
      "Epoch count 50: Loss: 540.7681274414062\n",
      "Epoch count 51: Loss: 502.63409423828125\n",
      "Epoch count 52: Loss: 542.7332763671875\n",
      "Epoch count 53: Loss: 562.1357421875\n",
      "Epoch count 54: Loss: 579.3878784179688\n",
      "Epoch count 55: Loss: 540.3745727539062\n",
      "Epoch count 56: Loss: 536.060546875\n",
      "Epoch count 57: Loss: 533.753662109375\n",
      "Epoch count 58: Loss: 533.148193359375\n",
      "Epoch count 59: Loss: 527.1832275390625\n",
      "Epoch count 60: Loss: 509.6258239746094\n",
      "Epoch count 61: Loss: 528.5675659179688\n",
      "Epoch count 62: Loss: 512.1214599609375\n",
      "Epoch count 63: Loss: 523.6930541992188\n",
      "Epoch count 64: Loss: 511.8440246582031\n",
      "Epoch count 65: Loss: 516.8753662109375\n",
      "Epoch count 66: Loss: 517.7816162109375\n",
      "Epoch count 67: Loss: 506.9337463378906\n",
      "Epoch count 68: Loss: 513.4520874023438\n",
      "Epoch count 69: Loss: 530.1843872070312\n",
      "Epoch count 70: Loss: 521.8470458984375\n",
      "Epoch count 71: Loss: 489.29205322265625\n",
      "Epoch count 72: Loss: 520.0505981445312\n",
      "Epoch count 73: Loss: 486.6424560546875\n",
      "Epoch count 74: Loss: 498.7767639160156\n",
      "Epoch count 75: Loss: 486.8253173828125\n",
      "Epoch count 76: Loss: 497.0891418457031\n",
      "Epoch count 77: Loss: 506.0698547363281\n",
      "Epoch count 78: Loss: 481.9829406738281\n",
      "Epoch count 79: Loss: 506.75775146484375\n",
      "Epoch count 80: Loss: 485.47515869140625\n",
      "Epoch count 81: Loss: 516.9905395507812\n",
      "Epoch count 82: Loss: 490.65362548828125\n",
      "Epoch count 83: Loss: 507.873779296875\n",
      "Epoch count 84: Loss: 498.4149475097656\n",
      "Epoch count 85: Loss: 485.6910095214844\n",
      "Epoch count 86: Loss: 500.2825622558594\n",
      "Epoch count 87: Loss: 492.1854553222656\n",
      "Epoch count 88: Loss: 498.0949401855469\n",
      "Epoch count 89: Loss: 490.60009765625\n",
      "Epoch count 90: Loss: 512.3655395507812\n",
      "Epoch count 91: Loss: 512.9689331054688\n",
      "Epoch count 92: Loss: 496.08685302734375\n",
      "Epoch count 93: Loss: 505.8321838378906\n",
      "Epoch count 94: Loss: 497.6875\n",
      "Epoch count 95: Loss: 496.8606872558594\n",
      "Epoch count 96: Loss: 499.59857177734375\n",
      "Epoch count 97: Loss: 499.96954345703125\n",
      "Epoch count 98: Loss: 497.8066711425781\n",
      "Epoch count 99: Loss: 497.4365539550781\n",
      "Epoch count 100: Loss: 490.9499816894531\n",
      "Epoch count 101: Loss: 497.6890563964844\n",
      "Epoch count 102: Loss: 491.0743713378906\n",
      "Epoch count 103: Loss: 493.37994384765625\n",
      "Epoch count 104: Loss: 496.6557922363281\n",
      "Epoch count 105: Loss: 493.153076171875\n",
      "Epoch count 106: Loss: 492.4935302734375\n",
      "Epoch count 107: Loss: 489.2242431640625\n",
      "Epoch count 108: Loss: 497.2644348144531\n",
      "Epoch count 109: Loss: 492.871337890625\n",
      "Epoch count 110: Loss: 490.8288879394531\n",
      "Epoch count 111: Loss: 490.0649108886719\n",
      "Epoch count 112: Loss: 495.1354064941406\n",
      "Epoch count 113: Loss: 490.19921875\n",
      "Epoch count 114: Loss: 490.32720947265625\n",
      "Epoch count 115: Loss: 495.4004821777344\n",
      "Epoch count 116: Loss: 492.8080139160156\n",
      "Epoch count 117: Loss: 491.8447265625\n",
      "Epoch count 118: Loss: 491.2613525390625\n",
      "Epoch count 119: Loss: 491.18438720703125\n",
      "Epoch count 120: Loss: 493.7776794433594\n",
      "Epoch count 121: Loss: 492.3785705566406\n",
      "Epoch count 122: Loss: 491.4454345703125\n",
      "Epoch count 123: Loss: 492.1276550292969\n",
      "Epoch count 124: Loss: 492.1942443847656\n",
      "Epoch count 125: Loss: 493.78765869140625\n",
      "Epoch count 126: Loss: 493.1790771484375\n",
      "Epoch count 127: Loss: 492.34454345703125\n",
      "Epoch count 128: Loss: 491.047119140625\n",
      "Epoch count 129: Loss: 491.9655456542969\n",
      "Epoch count 130: Loss: 490.01385498046875\n",
      "Epoch count 131: Loss: 491.4026794433594\n",
      "Epoch count 132: Loss: 492.3275146484375\n",
      "Epoch count 133: Loss: 491.1368103027344\n",
      "Epoch count 134: Loss: 494.17242431640625\n",
      "Epoch count 135: Loss: 492.06085205078125\n",
      "Epoch count 136: Loss: 491.4670104980469\n",
      "Epoch count 137: Loss: 490.4488830566406\n",
      "Epoch count 138: Loss: 488.6968994140625\n",
      "Epoch count 139: Loss: 492.2213439941406\n",
      "Epoch count 140: Loss: 490.9706726074219\n",
      "Epoch count 141: Loss: 491.7934875488281\n",
      "Epoch count 142: Loss: 492.7977600097656\n",
      "Epoch count 143: Loss: 492.25604248046875\n",
      "Epoch count 144: Loss: 492.62396240234375\n",
      "Epoch count 145: Loss: 491.3614196777344\n",
      "Epoch count 146: Loss: 491.4657897949219\n",
      "Epoch count 147: Loss: 491.6490783691406\n",
      "Epoch count 148: Loss: 492.4271240234375\n",
      "Epoch count 149: Loss: 491.10797119140625\n",
      "Epoch count 150: Loss: 491.99285888671875\n",
      "Epoch count 151: Loss: 491.6305236816406\n",
      "Epoch count 152: Loss: 491.0957336425781\n",
      "Epoch count 153: Loss: 491.881591796875\n",
      "Epoch count 154: Loss: 491.1742248535156\n",
      "Epoch count 155: Loss: 491.0119323730469\n",
      "Epoch count 156: Loss: 491.245361328125\n",
      "Epoch count 157: Loss: 491.4789123535156\n",
      "Epoch count 158: Loss: 490.9324951171875\n",
      "Epoch count 159: Loss: 491.0971984863281\n",
      "Epoch count 160: Loss: 490.780517578125\n",
      "Epoch count 161: Loss: 491.1855163574219\n",
      "Epoch count 162: Loss: 491.1225280761719\n",
      "Epoch count 163: Loss: 490.65869140625\n",
      "Epoch count 164: Loss: 490.4010009765625\n",
      "Epoch count 165: Loss: 490.30096435546875\n",
      "Epoch count 166: Loss: 490.42108154296875\n",
      "Epoch count 167: Loss: 489.9249267578125\n",
      "Epoch count 168: Loss: 490.3445129394531\n",
      "Epoch count 169: Loss: 491.34466552734375\n",
      "Epoch count 170: Loss: 489.63287353515625\n",
      "Epoch count 171: Loss: 490.26678466796875\n",
      "Epoch count 172: Loss: 489.4371337890625\n",
      "Epoch count 173: Loss: 493.6950988769531\n",
      "Epoch count 174: Loss: 489.0580139160156\n",
      "Epoch count 175: Loss: 491.0084228515625\n",
      "Epoch count 176: Loss: 489.45135498046875\n",
      "Epoch count 177: Loss: 488.9242858886719\n",
      "Epoch count 178: Loss: 492.7489013671875\n",
      "Epoch count 179: Loss: 488.4476318359375\n",
      "Epoch count 180: Loss: 488.6624450683594\n",
      "Epoch count 181: Loss: 489.91888427734375\n",
      "Epoch count 182: Loss: 490.62017822265625\n",
      "Epoch count 183: Loss: 491.4715576171875\n",
      "Epoch count 184: Loss: 493.22509765625\n",
      "Epoch count 185: Loss: 494.99212646484375\n",
      "Epoch count 186: Loss: 490.1485595703125\n",
      "Epoch count 187: Loss: 491.00048828125\n",
      "Epoch count 188: Loss: 490.15814208984375\n",
      "Epoch count 189: Loss: 491.00164794921875\n",
      "Epoch count 190: Loss: 490.5851745605469\n",
      "Epoch count 191: Loss: 492.1903381347656\n",
      "Epoch count 192: Loss: 491.3678894042969\n",
      "Epoch count 193: Loss: 490.3889465332031\n",
      "Epoch count 194: Loss: 490.5433044433594\n",
      "Epoch count 195: Loss: 490.9420471191406\n",
      "Epoch count 196: Loss: 490.6231384277344\n",
      "Epoch count 197: Loss: 490.562255859375\n",
      "Epoch count 198: Loss: 490.0303649902344\n",
      "Epoch count 199: Loss: 490.5462951660156\n",
      "Epoch count 200: Loss: 490.203857421875\n",
      "Epoch count 201: Loss: 490.73406982421875\n",
      "Epoch count 202: Loss: 490.0814208984375\n",
      "Epoch count 203: Loss: 490.1200866699219\n",
      "Epoch count 204: Loss: 489.2890625\n",
      "Epoch count 205: Loss: 490.2760314941406\n",
      "Epoch count 206: Loss: 490.42474365234375\n",
      "Epoch count 207: Loss: 491.1962585449219\n",
      "Epoch count 208: Loss: 490.25457763671875\n",
      "Epoch count 209: Loss: 489.3690185546875\n",
      "Epoch count 210: Loss: 489.3920593261719\n",
      "Epoch count 211: Loss: 489.8425598144531\n",
      "Epoch count 212: Loss: 489.3621520996094\n",
      "Epoch count 213: Loss: 489.890625\n",
      "Epoch count 214: Loss: 488.9472351074219\n",
      "Epoch count 215: Loss: 489.8941650390625\n",
      "Epoch count 216: Loss: 489.4723815917969\n",
      "Epoch count 217: Loss: 490.97161865234375\n",
      "Epoch count 218: Loss: 489.0924377441406\n",
      "Epoch count 219: Loss: 490.3642578125\n",
      "Epoch count 220: Loss: 488.5896301269531\n",
      "Epoch count 221: Loss: 488.8364562988281\n",
      "Epoch count 222: Loss: 490.86749267578125\n",
      "Epoch count 223: Loss: 488.3992919921875\n",
      "Epoch count 224: Loss: 488.434814453125\n",
      "Epoch count 225: Loss: 487.2584533691406\n",
      "Epoch count 226: Loss: 489.1075134277344\n",
      "Epoch count 227: Loss: 490.0118408203125\n",
      "Epoch count 228: Loss: 486.5480651855469\n",
      "Epoch count 229: Loss: 491.58074951171875\n",
      "Epoch count 230: Loss: 489.13580322265625\n",
      "Epoch count 231: Loss: 489.3378601074219\n",
      "Epoch count 232: Loss: 489.5805969238281\n",
      "Epoch count 233: Loss: 488.6561584472656\n",
      "Epoch count 234: Loss: 488.40338134765625\n",
      "Epoch count 235: Loss: 489.2687072753906\n",
      "Epoch count 236: Loss: 491.77825927734375\n",
      "Epoch count 237: Loss: 487.53558349609375\n",
      "Epoch count 238: Loss: 494.21807861328125\n",
      "Epoch count 239: Loss: 490.9476013183594\n",
      "Epoch count 240: Loss: 489.6357421875\n",
      "Epoch count 241: Loss: 490.9377136230469\n",
      "Epoch count 242: Loss: 490.1322326660156\n",
      "Epoch count 243: Loss: 489.6906433105469\n",
      "Epoch count 244: Loss: 490.39715576171875\n",
      "Epoch count 245: Loss: 489.77392578125\n",
      "Epoch count 246: Loss: 488.29547119140625\n",
      "Epoch count 247: Loss: 489.69970703125\n",
      "Epoch count 248: Loss: 491.0069274902344\n",
      "Epoch count 249: Loss: 489.1913146972656\n",
      "Epoch count 250: Loss: 489.2036437988281\n",
      "Epoch count 251: Loss: 489.3177795410156\n",
      "Epoch count 252: Loss: 488.9340515136719\n",
      "Epoch count 253: Loss: 488.2022399902344\n",
      "Epoch count 254: Loss: 488.9417724609375\n",
      "Epoch count 255: Loss: 488.1826171875\n",
      "Epoch count 256: Loss: 491.1742858886719\n",
      "Epoch count 257: Loss: 488.5347900390625\n",
      "Epoch count 258: Loss: 488.95294189453125\n",
      "Epoch count 259: Loss: 489.73223876953125\n",
      "Epoch count 260: Loss: 489.9085998535156\n",
      "Epoch count 261: Loss: 488.4720458984375\n",
      "Epoch count 262: Loss: 489.1455993652344\n",
      "Epoch count 263: Loss: 488.88494873046875\n",
      "Epoch count 264: Loss: 489.6111755371094\n",
      "Epoch count 265: Loss: 489.94317626953125\n",
      "Epoch count 266: Loss: 489.0789794921875\n",
      "Epoch count 267: Loss: 488.9582214355469\n",
      "Epoch count 268: Loss: 488.4551086425781\n",
      "Epoch count 269: Loss: 489.3258972167969\n",
      "Epoch count 270: Loss: 488.8875427246094\n",
      "Epoch count 271: Loss: 488.940673828125\n",
      "Epoch count 272: Loss: 487.501953125\n",
      "Epoch count 273: Loss: 488.600341796875\n",
      "Epoch count 274: Loss: 489.0381164550781\n",
      "Epoch count 275: Loss: 489.1268005371094\n",
      "Epoch count 276: Loss: 489.7686462402344\n",
      "Epoch count 277: Loss: 488.7419128417969\n",
      "Epoch count 278: Loss: 489.54150390625\n",
      "Epoch count 279: Loss: 487.7809753417969\n",
      "Epoch count 280: Loss: 487.7776184082031\n",
      "Epoch count 281: Loss: 490.251220703125\n",
      "Epoch count 282: Loss: 488.99371337890625\n",
      "Epoch count 283: Loss: 487.8617248535156\n",
      "Epoch count 284: Loss: 489.1270751953125\n",
      "Epoch count 285: Loss: 489.16107177734375\n",
      "Epoch count 286: Loss: 488.7490234375\n",
      "Epoch count 287: Loss: 487.75555419921875\n",
      "Epoch count 288: Loss: 488.53411865234375\n",
      "Epoch count 289: Loss: 488.1062927246094\n",
      "Epoch count 290: Loss: 488.9366455078125\n",
      "Epoch count 291: Loss: 488.8123474121094\n",
      "Epoch count 292: Loss: 487.9210205078125\n",
      "Epoch count 293: Loss: 488.8439025878906\n",
      "Epoch count 294: Loss: 487.6258239746094\n",
      "Epoch count 295: Loss: 488.0379638671875\n",
      "Epoch count 296: Loss: 486.78955078125\n",
      "Epoch count 297: Loss: 488.963623046875\n",
      "Epoch count 298: Loss: 488.9033203125\n",
      "Epoch count 299: Loss: 488.9233703613281\n",
      "Epoch count 300: Loss: 487.0011291503906\n",
      "Epoch count 301: Loss: 488.0491027832031\n",
      "Epoch count 302: Loss: 489.3208312988281\n",
      "Epoch count 303: Loss: 488.9873962402344\n",
      "Epoch count 304: Loss: 487.12237548828125\n",
      "Epoch count 305: Loss: 488.140869140625\n",
      "Epoch count 306: Loss: 488.2744140625\n",
      "Epoch count 307: Loss: 487.8099365234375\n",
      "Epoch count 308: Loss: 490.101318359375\n",
      "Epoch count 309: Loss: 488.010986328125\n",
      "Epoch count 310: Loss: 487.3663024902344\n",
      "Epoch count 311: Loss: 488.9093017578125\n",
      "Epoch count 312: Loss: 488.99560546875\n",
      "Epoch count 313: Loss: 487.3282775878906\n",
      "Epoch count 314: Loss: 488.2748107910156\n",
      "Epoch count 315: Loss: 488.3075256347656\n",
      "Epoch count 316: Loss: 486.5500793457031\n",
      "Epoch count 317: Loss: 488.9764099121094\n",
      "Epoch count 318: Loss: 488.3940124511719\n",
      "Epoch count 319: Loss: 488.4055480957031\n",
      "Epoch count 320: Loss: 487.713134765625\n",
      "Epoch count 321: Loss: 488.44268798828125\n",
      "Epoch count 322: Loss: 489.255615234375\n",
      "Epoch count 323: Loss: 487.7785339355469\n",
      "Epoch count 324: Loss: 487.9612121582031\n",
      "Epoch count 325: Loss: 487.1943054199219\n",
      "Epoch count 326: Loss: 487.2392578125\n",
      "Epoch count 327: Loss: 487.45538330078125\n",
      "Epoch count 328: Loss: 487.6469421386719\n",
      "Epoch count 329: Loss: 486.9197998046875\n",
      "Epoch count 330: Loss: 487.2698059082031\n",
      "Epoch count 331: Loss: 488.4051513671875\n",
      "Epoch count 332: Loss: 488.68121337890625\n",
      "Epoch count 333: Loss: 487.38787841796875\n",
      "Epoch count 334: Loss: 488.4239807128906\n",
      "Epoch count 335: Loss: 487.14508056640625\n",
      "Epoch count 336: Loss: 488.3282775878906\n",
      "Epoch count 337: Loss: 487.4991760253906\n",
      "Epoch count 338: Loss: 488.2218017578125\n",
      "Epoch count 339: Loss: 487.4307556152344\n",
      "Epoch count 340: Loss: 487.371337890625\n",
      "Epoch count 341: Loss: 486.8216857910156\n",
      "Epoch count 342: Loss: 487.1747131347656\n",
      "Epoch count 343: Loss: 487.5497131347656\n",
      "Epoch count 344: Loss: 486.76226806640625\n",
      "Epoch count 345: Loss: 485.76971435546875\n",
      "Epoch count 346: Loss: 485.9022521972656\n",
      "Epoch count 347: Loss: 485.593994140625\n",
      "Epoch count 348: Loss: 487.216552734375\n",
      "Epoch count 349: Loss: 488.2000427246094\n",
      "Epoch count 350: Loss: 486.9534606933594\n",
      "Epoch count 351: Loss: 489.26959228515625\n",
      "Epoch count 352: Loss: 487.0460510253906\n",
      "Epoch count 353: Loss: 487.4588928222656\n",
      "Epoch count 354: Loss: 486.712646484375\n",
      "Epoch count 355: Loss: 485.90484619140625\n",
      "Epoch count 356: Loss: 485.0166320800781\n",
      "Epoch count 357: Loss: 485.60638427734375\n",
      "Epoch count 358: Loss: 487.61334228515625\n",
      "Epoch count 359: Loss: 486.06591796875\n",
      "Epoch count 360: Loss: 489.1388854980469\n",
      "Epoch count 361: Loss: 486.7825622558594\n",
      "Epoch count 362: Loss: 487.62017822265625\n",
      "Epoch count 363: Loss: 486.18768310546875\n",
      "Epoch count 364: Loss: 487.5872497558594\n",
      "Epoch count 365: Loss: 486.4739074707031\n",
      "Epoch count 366: Loss: 486.7418212890625\n",
      "Epoch count 367: Loss: 487.52984619140625\n",
      "Epoch count 368: Loss: 485.3934326171875\n",
      "Epoch count 369: Loss: 485.9659729003906\n",
      "Epoch count 370: Loss: 488.0021667480469\n",
      "Epoch count 371: Loss: 488.6550598144531\n",
      "Epoch count 372: Loss: 487.56256103515625\n",
      "Epoch count 373: Loss: 488.1187744140625\n",
      "Epoch count 374: Loss: 488.3465270996094\n",
      "Epoch count 375: Loss: 488.0136413574219\n",
      "Epoch count 376: Loss: 485.5259094238281\n",
      "Epoch count 377: Loss: 486.2722473144531\n",
      "Epoch count 378: Loss: 487.17315673828125\n",
      "Epoch count 379: Loss: 489.9737548828125\n",
      "Epoch count 380: Loss: 487.742919921875\n",
      "Epoch count 381: Loss: 486.7257385253906\n",
      "Epoch count 382: Loss: 487.0504455566406\n",
      "Epoch count 383: Loss: 486.6083679199219\n",
      "Epoch count 384: Loss: 486.443603515625\n",
      "Epoch count 385: Loss: 486.38525390625\n",
      "Epoch count 386: Loss: 485.927734375\n",
      "Epoch count 387: Loss: 488.412841796875\n",
      "Epoch count 388: Loss: 487.125244140625\n",
      "Epoch count 389: Loss: 486.7942199707031\n",
      "Epoch count 390: Loss: 486.52099609375\n",
      "Epoch count 391: Loss: 486.32257080078125\n",
      "Epoch count 392: Loss: 486.3871765136719\n",
      "Epoch count 393: Loss: 485.69580078125\n",
      "Epoch count 394: Loss: 486.57891845703125\n",
      "Epoch count 395: Loss: 487.3918762207031\n",
      "Epoch count 396: Loss: 485.66796875\n",
      "Epoch count 397: Loss: 486.5015869140625\n",
      "Epoch count 398: Loss: 486.4231262207031\n",
      "Epoch count 399: Loss: 486.5655212402344\n",
      "Epoch count 400: Loss: 486.4883728027344\n",
      "Epoch count 401: Loss: 484.87451171875\n",
      "Epoch count 402: Loss: 487.1136169433594\n",
      "Epoch count 403: Loss: 485.6941833496094\n",
      "Epoch count 404: Loss: 486.377197265625\n",
      "Epoch count 405: Loss: 485.3600769042969\n",
      "Epoch count 406: Loss: 486.3385009765625\n",
      "Epoch count 407: Loss: 484.10137939453125\n",
      "Epoch count 408: Loss: 488.32012939453125\n",
      "Epoch count 409: Loss: 485.56170654296875\n",
      "Epoch count 410: Loss: 487.8724060058594\n",
      "Epoch count 411: Loss: 487.0219421386719\n",
      "Epoch count 412: Loss: 484.39935302734375\n",
      "Epoch count 413: Loss: 486.7878112792969\n",
      "Epoch count 414: Loss: 486.9035339355469\n",
      "Epoch count 415: Loss: 485.9130554199219\n",
      "Epoch count 416: Loss: 487.4825439453125\n",
      "Epoch count 417: Loss: 486.6491394042969\n",
      "Epoch count 418: Loss: 486.5498962402344\n",
      "Epoch count 419: Loss: 486.3211669921875\n",
      "Epoch count 420: Loss: 486.17242431640625\n",
      "Epoch count 421: Loss: 485.9573059082031\n",
      "Epoch count 422: Loss: 485.9844665527344\n",
      "Epoch count 423: Loss: 485.8699951171875\n",
      "Epoch count 424: Loss: 485.9933166503906\n",
      "Epoch count 425: Loss: 486.0592956542969\n",
      "Epoch count 426: Loss: 485.7003479003906\n",
      "Epoch count 427: Loss: 485.6644592285156\n",
      "Epoch count 428: Loss: 485.7718811035156\n",
      "Epoch count 429: Loss: 485.3776550292969\n",
      "Epoch count 430: Loss: 486.34454345703125\n",
      "Epoch count 431: Loss: 485.958251953125\n",
      "Epoch count 432: Loss: 484.9621276855469\n",
      "Epoch count 433: Loss: 486.3690185546875\n",
      "Epoch count 434: Loss: 486.308349609375\n",
      "Epoch count 435: Loss: 485.4396667480469\n",
      "Epoch count 436: Loss: 485.51361083984375\n",
      "Epoch count 437: Loss: 485.92919921875\n",
      "Epoch count 438: Loss: 484.91070556640625\n",
      "Epoch count 439: Loss: 484.6321105957031\n",
      "Epoch count 440: Loss: 486.93603515625\n",
      "Epoch count 441: Loss: 484.5355529785156\n",
      "Epoch count 442: Loss: 486.19091796875\n",
      "Epoch count 443: Loss: 483.2842102050781\n",
      "Epoch count 444: Loss: 486.4459533691406\n",
      "Epoch count 445: Loss: 486.3835144042969\n",
      "Epoch count 446: Loss: 485.3017578125\n",
      "Epoch count 447: Loss: 486.1402282714844\n",
      "Epoch count 448: Loss: 485.3882751464844\n",
      "Epoch count 449: Loss: 485.3296813964844\n",
      "Epoch count 450: Loss: 485.9258117675781\n",
      "Epoch count 451: Loss: 486.4490966796875\n",
      "Epoch count 452: Loss: 485.281005859375\n",
      "Epoch count 453: Loss: 484.5184326171875\n",
      "Epoch count 454: Loss: 485.31182861328125\n",
      "Epoch count 455: Loss: 485.05328369140625\n",
      "Epoch count 456: Loss: 485.58746337890625\n",
      "Epoch count 457: Loss: 487.8356628417969\n",
      "Epoch count 458: Loss: 485.11016845703125\n",
      "Epoch count 459: Loss: 484.70550537109375\n",
      "Epoch count 460: Loss: 484.7755432128906\n",
      "Epoch count 461: Loss: 483.203125\n",
      "Epoch count 462: Loss: 484.46392822265625\n",
      "Epoch count 463: Loss: 486.0037841796875\n",
      "Epoch count 464: Loss: 484.13385009765625\n",
      "Epoch count 465: Loss: 481.2781982421875\n",
      "Epoch count 466: Loss: 486.6500244140625\n",
      "Epoch count 467: Loss: 486.7629089355469\n",
      "Epoch count 468: Loss: 486.85064697265625\n",
      "Epoch count 469: Loss: 485.9638977050781\n",
      "Epoch count 470: Loss: 485.4259033203125\n",
      "Epoch count 471: Loss: 486.736328125\n",
      "Epoch count 472: Loss: 484.84228515625\n",
      "Epoch count 473: Loss: 484.93707275390625\n",
      "Epoch count 474: Loss: 485.0342712402344\n",
      "Epoch count 475: Loss: 484.4814453125\n",
      "Epoch count 476: Loss: 484.4198913574219\n",
      "Epoch count 477: Loss: 485.4354553222656\n",
      "Epoch count 478: Loss: 484.57196044921875\n",
      "Epoch count 479: Loss: 484.6924133300781\n",
      "Epoch count 480: Loss: 484.3347473144531\n",
      "Epoch count 481: Loss: 484.871826171875\n",
      "Epoch count 482: Loss: 484.11663818359375\n",
      "Epoch count 483: Loss: 483.9661560058594\n",
      "Epoch count 484: Loss: 483.5921936035156\n",
      "Epoch count 485: Loss: 485.16546630859375\n",
      "Epoch count 486: Loss: 484.0216369628906\n",
      "Epoch count 487: Loss: 484.69049072265625\n",
      "Epoch count 488: Loss: 487.1994934082031\n",
      "Epoch count 489: Loss: 484.47650146484375\n",
      "Epoch count 490: Loss: 484.95208740234375\n",
      "Epoch count 491: Loss: 485.7803955078125\n",
      "Epoch count 492: Loss: 484.1073913574219\n",
      "Epoch count 493: Loss: 485.5509948730469\n",
      "Epoch count 494: Loss: 483.97454833984375\n",
      "Epoch count 495: Loss: 484.9674987792969\n",
      "Epoch count 496: Loss: 486.4507751464844\n",
      "Epoch count 497: Loss: 484.64776611328125\n",
      "Epoch count 498: Loss: 484.86187744140625\n",
      "Epoch count 499: Loss: 484.671142578125\n",
      "Weight: [0.9985651 2.0005376], Bias: 22.015247344970703\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression project with generalized input dimensions and L2 normalisation\n",
    "__author__ = \"Billy Cao\"\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "import generator\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)  # change to false when running on ML server\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "batchSize = 100\n",
    "gen = generator.gen2d(batchSize)  # gen2d for 2D input\n",
    "epochs = 500\n",
    "regTerm = 0.001\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "class LinearModel:  # initializing to 1 now, but also can do 0 or random\n",
    "    def __call__(self, x):  # predicting function\n",
    "        return self.Weight * x + self.Bias\n",
    "\n",
    "    def __init__(self):\n",
    "        self.Weight = tf.Variable(1.0, shape=tf.TensorShape(None))  # initialize m to any shape\n",
    "        self.Bias = tf.Variable(1.0)\n",
    "\n",
    "\n",
    "def loss(y, pred):  # Mean Squared Error with L2 Normalisation\n",
    "    return tf.reduce_mean(tf.square(y - pred)) + tf.reduce_sum(regTerm * tf.square(linear_model.Weight))\n",
    "\n",
    "\n",
    "def train(linear_model, x, y, lr):\n",
    "    # use to reshape into 2D vector if input is of unknown dimension\n",
    "    # if len(x.shape) == 1:\n",
    "    #     X = tf.reshape(x, [x.shape[0], 1])\n",
    "    with tf.GradientTape(persistent=False) as t:  # persistent=True is needed if assigning dy_dWeight, dy_dBias in 2 lines. Limits the times u can call it to once\n",
    "        current_loss = loss(y, linear_model(x))\n",
    "    dy_dWeight, dy_dBias = t.gradient(current_loss, [linear_model.Weight, linear_model.Bias])\n",
    "    linear_model.Weight.assign_sub(lr * dy_dWeight)\n",
    "    linear_model.Bias.assign_sub(lr * dy_dBias)\n",
    "\n",
    "\n",
    "linear_model = LinearModel()\n",
    "sampleX, sampleY = next(gen)\n",
    "linear_model.Weight.assign([1.0] * sampleX.shape[-1])  # initialize m to 1.0 and make it same dimension as the input\n",
    "for epoch_count in range(epochs):\n",
    "    x, y = next(gen)\n",
    "    real_loss = loss(y, linear_model(x))\n",
    "    train(linear_model, x, y, lr=learning_rate)\n",
    "    print(f\"Epoch count {epoch_count}: Loss: {real_loss.numpy()}\")\n",
    "\n",
    "# Assuming the weights are correct, the loss will be directly related to error in Bias. Thus we compensate it here\n",
    "linear_model.Bias.assign_sub(linear_model.Bias - sqrt(real_loss.numpy()))\n",
    "print(f'Weight: {linear_model.Weight.numpy()}, Bias: {linear_model.Bias.numpy()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}