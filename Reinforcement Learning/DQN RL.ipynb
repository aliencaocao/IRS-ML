{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Tensorflow 2.4.1 and Python 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import q_network, sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "imageio.plugins.ffmpeg.download()\n",
    "print(f'Running on Tensorflow {tf.__version__} and Python {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 无法在设置线程模式后对其加以更改。\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=600x400 at 0x22086A7B220>",
      "image/png": "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\n"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations = 20000\n",
    "initial_collect_steps = 100\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "log_interval = 200\n",
    "num_eval_episodes = 100\n",
    "eval_interval = 500\n",
    "dense_layer_params = (100, 50)  # no of units for layers. Add more numbers to increase no of layers\n",
    "\n",
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
      "\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "\n",
      "Time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.04766495, -0.032955  ,  0.04077735, -0.02377388], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.04832405, -0.2286373 ,  0.04030187,  0.28149077], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1)})\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('\\nReward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "print('\\nAction Spec:')\n",
    "print(env.action_spec())\n",
    "time_step = env.reset()\n",
    "print('\\nTime step:')\n",
    "print(time_step)\n",
    "# carpole: 0 - left, 1 - right, guess why cannot stay still? MountainCar: 0-left, 1-still, 2-right\n",
    "action = np.array(0, dtype=np.int32)\n",
    "next_time_step = env.step(action)\n",
    "print('\\nNext time step:')\n",
    "print(next_time_step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Two env are made, one for training one for val\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "# Convert them to tensorflow env to run faster\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  120       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  1200      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  98        \n",
      "=================================================================\n",
      "Total params: 1,418\n",
      "Trainable params: 1,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": "<IPython.core.display.Image object>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DDQN Agent (Double DQN): Q-Network\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return Dense(num_units, activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer with `num_actions` units to generate one q_value per available action as it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in dense_layer_params]\n",
    "q_values_layer = Dense(num_actions, activation=None, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "                       bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "# Network needs observation not timestep because it is not supposed to know the answer at all\n",
    "# q_net = q_network.QNetwork(train_env.observation_spec(), train_env.action_spec(), fc_layer_params=dense_layer_params, activation_fn=tf.keras.activations.tanh, name='QNetwork')\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DdqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=opt,\n",
    "    gamma=0.95,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "print(q_net.summary())\n",
    "tf.keras.utils.plot_model(q_net, 'qnet.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(1, dtype=int64)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
      "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
      "      dtype=float32)),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "WARNING:tensorflow:From C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\operators\\control_flow.py:1218: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n"
     ]
    }
   ],
   "source": [
    "print(agent.collect_data_spec)\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # random movements\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# steps=2 here as we need both the current and future observation of the env to calculate loss\n",
    "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "iterator = iter(dataset)\n",
    "# print('Sample:', iterator.next(), sep='\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial return: 29.19\n",
      "WARNING:tensorflow:From C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 159.10940551757812\n",
      "step = 400: loss = 6.341194152832031\n",
      "step = 500: Average Return over 100 eps = 9.350000381469727\n",
      "step = 600: loss = 2.4527924060821533\n",
      "step = 800: loss = 1.1330645084381104\n",
      "step = 1000: loss = 0.4483502507209778\n",
      "step = 1000: Average Return over 100 eps = 23.649999618530273\n",
      "step = 1200: loss = 1.818756341934204\n",
      "step = 1400: loss = 0.8733325004577637\n",
      "step = 1500: Average Return over 100 eps = 87.45999908447266\n",
      "step = 1600: loss = 3.4736781120300293\n",
      "step = 1800: loss = 1.2506515979766846\n",
      "step = 2000: loss = 2.0719587802886963\n",
      "step = 2000: Average Return over 100 eps = 85.8499984741211\n",
      "step = 2200: loss = 4.774064064025879\n",
      "step = 2400: loss = 0.46761661767959595\n",
      "step = 2500: Average Return over 100 eps = 71.01000213623047\n",
      "step = 2600: loss = 4.279891490936279\n",
      "step = 2800: loss = 0.3739345967769623\n",
      "step = 3000: loss = 0.5621891617774963\n",
      "step = 3000: Average Return over 100 eps = 89.13999938964844\n",
      "step = 3200: loss = 0.5209585428237915\n",
      "step = 3400: loss = 1.099336862564087\n",
      "step = 3500: Average Return over 100 eps = 81.9800033569336\n",
      "step = 3600: loss = 0.35418057441711426\n",
      "step = 3800: loss = 0.9777126312255859\n",
      "step = 4000: loss = 0.5398315191268921\n",
      "step = 4000: Average Return over 100 eps = 100.41999816894531\n",
      "step = 4200: loss = 0.2563679814338684\n",
      "step = 4400: loss = 1.107026219367981\n",
      "step = 4500: Average Return over 100 eps = 95.36000061035156\n",
      "step = 4600: loss = 1.1186192035675049\n",
      "step = 4800: loss = 0.45829644799232483\n",
      "step = 5000: loss = 0.4694114625453949\n",
      "step = 5000: Average Return over 100 eps = 77.91999816894531\n",
      "step = 5200: loss = 0.7650927901268005\n",
      "step = 5400: loss = 12.552796363830566\n",
      "step = 5500: Average Return over 100 eps = 94.20999908447266\n",
      "step = 5600: loss = 0.33895978331565857\n",
      "step = 5800: loss = 0.4302263855934143\n",
      "step = 6000: loss = 0.6226069331169128\n",
      "step = 6000: Average Return over 100 eps = 141.22999572753906\n",
      "step = 6200: loss = 1.3148934841156006\n",
      "step = 6400: loss = 0.5146399736404419\n",
      "step = 6500: Average Return over 100 eps = 130.11000061035156\n",
      "step = 6600: loss = 0.2226855754852295\n",
      "step = 6800: loss = 0.25800472497940063\n",
      "step = 7000: loss = 0.45132070779800415\n",
      "step = 7000: Average Return over 100 eps = 143.0500030517578\n",
      "step = 7200: loss = 0.3249868154525757\n",
      "step = 7400: loss = 0.19590841233730316\n",
      "step = 7500: Average Return over 100 eps = 117.54000091552734\n",
      "step = 7600: loss = 0.6041774749755859\n",
      "step = 7800: loss = 0.167189821600914\n",
      "step = 8000: loss = 0.2700997591018677\n",
      "step = 8000: Average Return over 100 eps = 197.75\n",
      "step = 8200: loss = 0.1471296101808548\n",
      "step = 8400: loss = 0.19774630665779114\n",
      "step = 8500: Average Return over 100 eps = 194.2100067138672\n",
      "step = 8600: loss = 0.22277358174324036\n",
      "step = 8800: loss = 0.3698037564754486\n",
      "step = 9000: loss = 0.5475730895996094\n",
      "step = 9000: Average Return over 100 eps = 175.41000366210938\n",
      "step = 9200: loss = 0.8834277391433716\n",
      "step = 9400: loss = 0.24727214872837067\n",
      "step = 9500: Average Return over 100 eps = 199.5800018310547\n",
      "step = 9600: loss = 0.3282102346420288\n",
      "step = 9800: loss = 0.7664738893508911\n",
      "step = 10000: loss = 4.322378635406494\n",
      "step = 10000: Average Return over 100 eps = 199.49000549316406\n",
      "step = 10200: loss = 0.38817188143730164\n",
      "step = 10400: loss = 0.9477843046188354\n",
      "step = 10500: Average Return over 100 eps = 200.0\n",
      "step = 10600: loss = 10.615405082702637\n",
      "step = 10800: loss = 2.4722375869750977\n",
      "step = 11000: loss = 0.6747615337371826\n",
      "step = 11000: Average Return over 100 eps = 171.0399932861328\n",
      "step = 11200: loss = 5.5260491371154785\n",
      "step = 11400: loss = 5.781113147735596\n",
      "step = 11500: Average Return over 100 eps = 149.3300018310547\n",
      "step = 11600: loss = 0.32967686653137207\n",
      "step = 11800: loss = 0.3392791450023651\n",
      "step = 12000: loss = 0.34022390842437744\n",
      "step = 12000: Average Return over 100 eps = 194.80999755859375\n",
      "step = 12200: loss = 2.6383793354034424\n",
      "step = 12400: loss = 0.38328155875205994\n",
      "step = 12500: Average Return over 100 eps = 22.239999771118164\n",
      "step = 12600: loss = 0.35809123516082764\n",
      "step = 12800: loss = 0.3235778510570526\n",
      "step = 13000: loss = 0.8445067405700684\n",
      "step = 13000: Average Return over 100 eps = 16.459999084472656\n",
      "step = 13200: loss = 2.1006689071655273\n",
      "step = 13400: loss = 3.8555455207824707\n",
      "step = 13500: Average Return over 100 eps = 112.95999908447266\n",
      "step = 13600: loss = 5.023237705230713\n",
      "step = 13800: loss = 0.8267016410827637\n",
      "step = 14000: loss = 3.2339932918548584\n",
      "step = 14000: Average Return over 100 eps = 128.75999450683594\n",
      "step = 14200: loss = 3.577411651611328\n",
      "step = 14400: loss = 0.5720340013504028\n",
      "step = 14500: Average Return over 100 eps = 114.63999938964844\n",
      "step = 14600: loss = 0.6911715269088745\n",
      "step = 14800: loss = 0.4425291419029236\n",
      "step = 15000: loss = 1.4637867212295532\n",
      "step = 15000: Average Return over 100 eps = 34.459999084472656\n",
      "step = 15200: loss = 1.6820309162139893\n",
      "step = 15400: loss = 0.995357871055603\n",
      "step = 15500: Average Return over 100 eps = 123.30000305175781\n",
      "step = 15600: loss = 0.2252781093120575\n",
      "step = 15800: loss = 0.2814004421234131\n",
      "step = 16000: loss = 1.6447944641113281\n",
      "step = 16000: Average Return over 100 eps = 151.8699951171875\n",
      "step = 16200: loss = 1.9095377922058105\n",
      "step = 16400: loss = 2.4227869510650635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19616/1365304329.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0meval_interval\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m     \u001B[0mavg_return\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_avg_return\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meval_env\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolicy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_eval_episodes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'step = {step}: Average Return over {num_eval_episodes} eps = {avg_return}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[0mreturns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mavg_return\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_19616/1900241312.py\u001B[0m in \u001B[0;36mcompute_avg_return\u001B[1;34m(environment, policy, num_episodes)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_last\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m       \u001B[0maction_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m       \u001B[0mtime_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m       \u001B[0mepisode_return\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mtotal_return\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mepisode_return\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_environment.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    239\u001B[0m           \u001B[0mcorresponding\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mobservation_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m     \"\"\"\n\u001B[1;32m--> 241\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    242\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    243\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    319\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_time_step_dtypes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    320\u001B[0m           name='step_py_func')\n\u001B[1;32m--> 321\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_time_step_from_numpy_function_outputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    322\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    323\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mText\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'rgb_array'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNestedTensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_time_step_from_numpy_function_outputs\u001B[1;34m(self, outputs)\u001B[0m\n\u001B[0;32m    375\u001B[0m     \u001B[0mbatch_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatched\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    376\u001B[0m     \u001B[0mbatch_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensorShape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 377\u001B[1;33m     time_step = _pack_named_sequence(outputs,\n\u001B[0m\u001B[0;32m    378\u001B[0m                                      \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m                                      batch_shape)\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_pack_named_sequence\u001B[1;34m(flat_inputs, input_spec, batch_shape)\u001B[0m\n\u001B[0;32m     42\u001B[0m   \u001B[0mnamed_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m   \u001B[1;32mfor\u001B[0m \u001B[0mflat_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspec\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mflat_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_spec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m     \u001B[0mnamed_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mflat_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m       \u001B[0mnamed_input\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_shape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    199\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    200\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 201\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    202\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    203\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m    285\u001B[0m     \u001B[1;31m# variables. Variables have correct handle data when graph building.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    286\u001B[0m     \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 287\u001B[1;33m   \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    288\u001B[0m   \u001B[1;31m# Propagate handle data for happier shape inference for resource variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    289\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_handle_data\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m   3926\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3927\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3928\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   3929\u001B[0m         _ctx, \"Identity\", name, input)\n\u001B[0;32m   3930\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)  # Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train_step_counter.assign(0)  # Reset the train step\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print('Initial return:', avg_return)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, extra_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print(f'step = {step}: Average Return over {num_eval_episodes} eps = {avg_return}')\n",
    "    returns.append(avg_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise reward, max 200 for CartPole-v0\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "  return IPython.display.HTML(tag)\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_episodes=20, fps=60):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "# Generate results videos of trained and random\n",
    "create_policy_eval_video(agent.policy, f\"{env_name} trained-agent\")\n",
    "create_policy_eval_video(random_policy, f\"{env_name} random-agent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}