{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Tensorflow 2.5.0 and Python 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import sys\n",
    "import base64\n",
    "import imageio\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import sequential\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.utils import common\n",
    "imageio.plugins.ffmpeg.download()\n",
    "print(f'Running on Tensorflow {tf.__version__} and Python {sys.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] 无法在设置线程模式后对其加以更改。\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=600x400 at 0x26506354A00>",
      "image/png": "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\n"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "initial_collect_steps = 1000\n",
    "collect_steps_per_iteration = 1\n",
    "replay_buffer_max_length = 100000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "log_interval = 200\n",
    "num_eval_episodes = 10\n",
    "eval_interval = 500\n",
    "dense_layer_params = (200,100)  # no of units for layers. Add more numbers to increase no of layers\n",
    "\n",
    "env_name = 'MountainCar-v0'\n",
    "env = suite_gym.load(env_name)\n",
    "env.reset()\n",
    "PIL.Image.fromarray(env.render())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Spec:\n",
      "BoundedArraySpec(shape=(2,), dtype=dtype('float32'), name='observation', minimum=[-1.2  -0.07], maximum=[0.6  0.07])\n",
      "Reward Spec:\n",
      "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
      "Action Spec:\n",
      "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=2)\n",
      "Time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.4259871,  0.       ], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0)})\n",
      "Next time step:\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([-0.42670876, -0.00072167], dtype=float32),\n",
      " 'reward': array(-1., dtype=float32),\n",
      " 'step_type': array(1)})\n"
     ]
    }
   ],
   "source": [
    "print('Observation Spec:')\n",
    "print(env.time_step_spec().observation)\n",
    "print('Reward Spec:')\n",
    "print(env.time_step_spec().reward)\n",
    "print('Action Spec:')\n",
    "print(env.action_spec())\n",
    "time_step = env.reset()\n",
    "print('Time step:')\n",
    "print(time_step)\n",
    "action = np.array(1, dtype=np.int32)  # 0 - left, 1 - right for car pole\n",
    "next_time_step = env.step(action)\n",
    "print('Next time step:')\n",
    "print(next_time_step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Two env are made, one for training one for val\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "# Convert them to tensorflow env to run faster\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                multiple                  600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  303       \n",
      "=================================================================\n",
      "Total params: 21,003\n",
      "Trainable params: 21,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# DQN Agent: Q-Network\n",
    "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
    "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
    "\n",
    "# Define a helper function to create Dense layers configured with the right activation and kernel initializer.\n",
    "def dense_layer(num_units):\n",
    "  return Dense(num_units, activation='relu', kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
    "\n",
    "# QNetwork consists of a sequence of Dense layers followed by a dense layer with `num_actions` units to generate one q_value per available action as it's output.\n",
    "dense_layers = [dense_layer(num_units) for num_units in dense_layer_params]\n",
    "q_values_layer = Dense(num_actions, activation=None, kernel_initializer=tf.keras.initializers.RandomUniform(minval=-0.03, maxval=0.03),\n",
    "                       bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
    "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=opt,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    train_step_counter=train_step_counter)\n",
    "\n",
    "agent.initialize()\n",
    "print(q_net.summary())\n",
    "eval_policy = agent.policy\n",
    "collect_policy = agent.collect_policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory(\n",
      "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0, dtype=int64), maximum=array(2, dtype=int64)),\n",
      " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
      " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
      " 'observation': BoundedTensorSpec(shape=(2,), dtype=tf.float32, name='observation', minimum=array([-1.2 , -0.07], dtype=float32), maximum=array([0.6 , 0.07], dtype=float32)),\n",
      " 'policy_info': (),\n",
      " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
      " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
      "WARNING:tensorflow:From C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:382: ReplayBuffer.get_next (from tf_agents.replay_buffers.replay_buffer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `as_dataset(..., single_deterministic_pass=False) instead.\n",
      "Baseline result using random movements:\n"
     ]
    }
   ],
   "source": [
    "print(agent.collect_data_spec)\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_max_length)\n",
    "\n",
    "# steps=2 here as we need both the current and future observation of the env to calculate loss\n",
    "dataset = replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=batch_size, num_steps=2).prefetch(3)\n",
    "\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # random movements\n",
    "print('Baseline result using random movements:')\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
    "iterator = iter(dataset)\n",
    "# print(iterator.next())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial return: -200.0\n",
      "WARNING:tensorflow:From C:\\Users\\alien\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
      "Instead of:\n",
      "results = tf.foldr(fn, elems, back_prop=False)\n",
      "Use:\n",
      "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
      "step = 200: loss = 8.213010787963867\n",
      "step = 400: loss = 16.156780242919922\n",
      "step = 500: Average Return = -200.0\n",
      "step = 600: loss = 29.914958953857422\n",
      "step = 800: loss = 17.741792678833008\n",
      "step = 1000: loss = 24.571815490722656\n",
      "step = 1000: Average Return = -200.0\n",
      "step = 1200: loss = 14.441438674926758\n",
      "step = 1400: loss = 18.48244857788086\n",
      "step = 1500: Average Return = -200.0\n",
      "step = 1600: loss = 11.523893356323242\n",
      "step = 1800: loss = 8.19085693359375\n",
      "step = 2000: loss = 10.446535110473633\n",
      "step = 2000: Average Return = -200.0\n",
      "step = 2200: loss = 6.543125152587891\n",
      "step = 2400: loss = 9.993047714233398\n",
      "step = 2500: Average Return = -200.0\n",
      "step = 2600: loss = 3.929243326187134\n",
      "step = 2800: loss = 5.538171768188477\n",
      "step = 3000: loss = 3.337223768234253\n",
      "step = 3000: Average Return = -200.0\n",
      "step = 3200: loss = 4.71749210357666\n",
      "step = 3400: loss = 10.311134338378906\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29076/4113325885.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mstep\u001B[0m \u001B[1;33m%\u001B[0m \u001B[0meval_interval\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 23\u001B[1;33m     \u001B[0mavg_return\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_avg_return\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0meval_env\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0magent\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpolicy\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnum_eval_episodes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     24\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mf'step = {step}: Average Return = {avg_return}'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m     \u001B[0mreturns\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mavg_return\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29076/3146107916.py\u001B[0m in \u001B[0;36mcompute_avg_return\u001B[1;34m(environment, policy, num_episodes)\u001B[0m\n\u001B[0;32m      6\u001B[0m     \u001B[1;32mwhile\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_last\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m       \u001B[0maction_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpolicy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtime_step\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m       \u001B[0mtime_step\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0menvironment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m       \u001B[0mepisode_return\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mtime_step\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mtotal_return\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mepisode_return\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_environment.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    239\u001B[0m           \u001B[0mcorresponding\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mobservation_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m     \"\"\"\n\u001B[1;32m--> 241\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_step\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    242\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    243\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    319\u001B[0m           \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_time_step_dtypes\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    320\u001B[0m           name='step_py_func')\n\u001B[1;32m--> 321\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_time_step_from_numpy_function_outputs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    322\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    323\u001B[0m   \u001B[1;32mdef\u001B[0m \u001B[0mrender\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mText\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'rgb_array'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mNestedTensor\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_time_step_from_numpy_function_outputs\u001B[1;34m(self, outputs)\u001B[0m\n\u001B[0;32m    375\u001B[0m     \u001B[0mbatch_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatched\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    376\u001B[0m     \u001B[0mbatch_shape\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTensorShape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_shape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 377\u001B[1;33m     time_step = _pack_named_sequence(outputs,\n\u001B[0m\u001B[0;32m    378\u001B[0m                                      \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime_step_spec\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m                                      batch_shape)\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tf_agents\\environments\\tf_py_environment.py\u001B[0m in \u001B[0;36m_pack_named_sequence\u001B[1;34m(flat_inputs, input_spec, batch_shape)\u001B[0m\n\u001B[0;32m     42\u001B[0m   \u001B[0mnamed_inputs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m   \u001B[1;32mfor\u001B[0m \u001B[0mflat_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mspec\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mflat_inputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mnest\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput_spec\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m     \u001B[0mnamed_input\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mflat_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexecuting_eagerly\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m       \u001B[0mnamed_input\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_shape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbatch_shape\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mspec\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    204\u001B[0m     \u001B[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    205\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 206\u001B[1;33m       \u001B[1;32mreturn\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    207\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    208\u001B[0m       \u001B[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001B[0m in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m    286\u001B[0m     \u001B[1;31m# variables. Variables have correct handle data when graph building.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    287\u001B[0m     \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 288\u001B[1;33m   \u001B[0mret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgen_array_ops\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0midentity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    289\u001B[0m   \u001B[1;31m# Propagate handle data for happier shape inference for resource variables.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    290\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"_handle_data\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001B[0m in \u001B[0;36midentity\u001B[1;34m(input, name)\u001B[0m\n\u001B[0;32m   3946\u001B[0m   \u001B[1;32mif\u001B[0m \u001B[0mtld\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_eager\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3947\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3948\u001B[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001B[0m\u001B[0;32m   3949\u001B[0m         _ctx, \"Identity\", name, input)\n\u001B[0;32m   3950\u001B[0m       \u001B[1;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)  # Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train_step_counter.assign(0)  # Reset the train step\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "print('Initial return:', avg_return)\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, extra_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print(f'step = {step}: loss = {train_loss}')\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print(f'step = {step}: Average Return = {avg_return}')\n",
    "    returns.append(avg_return)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualise reward, max 200 for CartPole-v0\n",
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def embed_mp4(filename):\n",
    "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
    "  video = open(filename,'rb').read()\n",
    "  b64 = base64.b64encode(video)\n",
    "  tag = '''\n",
    "  <video width=\"640\" height=\"480\" controls>\n",
    "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "  </video>'''.format(b64.decode())\n",
    "\n",
    "  return IPython.display.HTML(tag)\n",
    "\n",
    "def create_policy_eval_video(policy, filename, num_episodes=10, fps=60):\n",
    "  filename = filename + \".mp4\"\n",
    "  with imageio.get_writer(filename, fps=fps) as video:\n",
    "    for _ in range(num_episodes):\n",
    "      time_step = eval_env.reset()\n",
    "      video.append_data(eval_py_env.render())\n",
    "      while not time_step.is_last():\n",
    "        action_step = policy.action(time_step)\n",
    "        time_step = eval_env.step(action_step.action)\n",
    "        video.append_data(eval_py_env.render())\n",
    "  return embed_mp4(filename)\n",
    "\n",
    "# Generate results videos of trained and random\n",
    "create_policy_eval_video(agent.policy, \"trained-agent\")\n",
    "create_policy_eval_video(random_policy, \"random-agent\", num_episodes=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}