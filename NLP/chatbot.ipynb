{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UZ0kXkJXKFlQ",
    "outputId": "2cfa98fc-88ff-40b9-eb17-c14f226b2463"
   },
   "source": [
    "#training\n",
    "\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timeframes = ['2008-01','2008-02','2008-03','2008-04','2008-05','2008-06','2008-07','2008-08','2008-09']\n",
    "main_df = pd.DataFrame(columns=['parent_id','comment_id','parent','comment','subreddit','unix','score'])\n",
    "droplist = ['https://','http://','www.','.com','.net','.org']\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
    "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "for timeframe in timeframes:\n",
    "    connection = sqlite3.connect('reddit_data/{}.db'.format(timeframe))\n",
    "    c = connection.cursor()\n",
    "    df = pd.read_sql('SELECT * FROM parent_reply', connection)\n",
    "    df = df[~df.parent.str.contains('|'.join(droplist))]\n",
    "    df = df[~df.comment.str.contains('|'.join(droplist))]\n",
    "    main_df = pd.concat([main_df, df])\n",
    "\n",
    "print('Dataframe loaded.')\n",
    "main_df = main_df.reset_index(drop=True)\n",
    "parent_df = main_df['parent']\n",
    "reply_df = main_df['comment']\n",
    "parent_df = parent_df.apply(preprocess_sentence)\n",
    "reply_df = reply_df.apply(preprocess_sentence)\n",
    "print('Dataframe cleaned.')\n",
    "\n",
    "'''HYPERPARAMS'''\n",
    "# Maximum sentence length\n",
    "MAX_LENGTH = 100\n",
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 200000\n",
    "# For tf.data.Dataset\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 20000\n",
    "# For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 60\n",
    "'''HYPERPARAMS'''\n",
    "\n",
    "print(parent_df[20])\n",
    "\n",
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = Tokenizer(num_words=1e5, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(parent_df+reply_df)\n",
    "word_index = tokenizer.word_index\n",
    "# tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(parent_df+reply_df, target_vocab_size=2**20)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [len(word_index)], [len(word_index) + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = len(word_index) + 2\n",
    "print('Tokenized sample question: {}'.format(tokenizer.texts_to_sequences(parent_df[20])))\n",
    "\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.texts_to_sequences(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.texts_to_sequences(sentence2) + END_TOKEN\n",
    "        # check tokenized sentence max length\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post', truncating='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "tk_parent_df, tk_reply_df = tokenize_and_filter(parent_df, reply_df)\n",
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(tk_parent_df)))\n",
    "\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'inputs': tk_parent_df,'dec_inputs': tk_reply_df[:, :-1]}, {'outputs': tk_reply_df[:, 1:]},\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "#ATTENTION\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.keras.layers.Lambda(lambda inputs: tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)\n",
    "        return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,\n",
    "                                                                                      (batch_size, -1, self.d_model)))(\n",
    "            scaled_attention)\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "#MASK\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model,\n",
    "\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # apply sin to even index in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd index in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "#ENCODER LAYER\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    add_attention = tf.keras.layers.add([inputs,attention])\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.add([attention,outputs])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "#ENCODER\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "      outputs = encoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "#DECODER LAYER\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "    add_attention = tf.keras.layers.add([attention1,inputs])\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    add_attention = tf.keras.layers.add([attention2,attention1])\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.add([outputs,attention2])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "#DECODER\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "\n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,)(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "#LOSS\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.constant(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model, \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))\n",
    "\n",
    "#TRAIN\n",
    "# clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-7)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=8, verbose=1,\n",
    "                                     mode='auto', baseline=None, restore_best_weights=True),\n",
    "    # tf.keras.callbacks.ModelCheckpoint('./best_model',monitor='val_accuracy',save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=5, verbose=1)\n",
    "]\n",
    "\n",
    "\n",
    "# initialize and compile model within strategy scope\n",
    "model = transformer(\n",
    "  vocab_size=VOCAB_SIZE,\n",
    "  num_layers=NUM_LAYERS,\n",
    "  units=UNITS,\n",
    "  d_model=D_MODEL,\n",
    "  num_heads=NUM_HEADS,\n",
    "  dropout=DROPOUT)\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model.summary()\n",
    "\n",
    "#TRAINING\n",
    "model.fit(dataset, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks, verbose=1)\n",
    "\n",
    "\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.texts_to_sequences(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # concatenated the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "    return predicted_sentence\n",
    "\n",
    "output = predict('Where have you been?')"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe loaded.\n",
      "Dataframe cleaned.\n",
      "i can has royaltyz ?\n",
      "Tokenized sample question: [[6], [], [220], [7], [616], [], [1123], [7], [41], [], [918], [491], [1020], [7], [1227], [493], [1020], [2561], [], []]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Python39\\lib\\site-packages\\keras_preprocessing\\sequence.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  sample_shape = np.asarray(x).shape[1:]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (13,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16348/3861102357.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    116\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 117\u001B[1;33m \u001B[0mtk_parent_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtk_reply_df\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtokenize_and_filter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparent_df\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreply_df\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    118\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Vocab size: {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mVOCAB_SIZE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'Number of samples: {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtk_parent_df\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16348/3861102357.py\u001B[0m in \u001B[0;36mtokenize_and_filter\u001B[1;34m(inputs, outputs)\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    108\u001B[0m     \u001B[1;31m# pad tokenized sentences\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 109\u001B[1;33m     tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n\u001B[0m\u001B[0;32m    110\u001B[0m         tokenized_inputs, maxlen=MAX_LENGTH, padding='post', truncating='post')\n\u001B[0;32m    111\u001B[0m     tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
      "\u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001B[0m in \u001B[0;36mpad_sequences\u001B[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001B[0m\n\u001B[0;32m    150\u001B[0m           \u001B[1;32mor\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mcase\u001B[0m \u001B[0mof\u001B[0m \u001B[0minvalid\u001B[0m \u001B[0mshape\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0ma\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0msequences\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0mentry\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    151\u001B[0m   \"\"\"\n\u001B[1;32m--> 152\u001B[1;33m   return sequence.pad_sequences(\n\u001B[0m\u001B[0;32m    153\u001B[0m       \u001B[0msequences\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmaxlen\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmaxlen\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    154\u001B[0m       padding=padding, truncating=truncating, value=value)\n",
      "\u001B[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001B[0m in \u001B[0;36mpad_sequences\u001B[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m         \u001B[1;31m# check `trunc` has expected shape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 98\u001B[1;33m         \u001B[0mtrunc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrunc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     99\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mtrunc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[0msample_shape\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    100\u001B[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (13,) + inhomogeneous part."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DKTBGIuOOuDY"
   },
   "source": [
    "model.save_weights('/content/drive/MyDrive/reddit_data/secondmodel.h5', overwrite=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40LHvY9Otpx1",
    "outputId": "ed0ecec1-6153-49fc-e5f8-a86107d1c22d"
   },
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "timeframes = ['2008-01','2008-02','2008-03','2008-04','2008-05','2008-06','2008-07','2008-08','2008-09']\n",
    "main_df = pd.DataFrame(columns=['parent_id','comment_id','parent','comment','subreddit','unix','score'])\n",
    "droplist = ['https://','http://','www.','.com','.net','.org']\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU {}'.format(tpu.cluster_spec().as_dict()['worker']))\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "print(\"REPLICAS: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "  sentence = sentence.lower().strip()\n",
    "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "  sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "  sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "  sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "  sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "  sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "  sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "  sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
    "  sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "  sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "  sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "  sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "  sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "  sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "  sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "  sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "  sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "  sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "  sentence = sentence.strip()\n",
    "  return sentence\n",
    "\n",
    "for timeframe in timeframes:\n",
    "    connection = sqlite3.connect('reddit_data/{}.db'.format(timeframe))\n",
    "    c = connection.cursor()\n",
    "    df = pd.read_sql('SELECT * FROM parent_reply', connection)\n",
    "    df = df[~df.parent.str.contains('|'.join(droplist))]\n",
    "    df = df[~df.comment.str.contains('|'.join(droplist))]\n",
    "    main_df = pd.concat([main_df, df])\n",
    "\n",
    "print('Dataframe loaded.')\n",
    "main_df = main_df.reset_index(drop=True)\n",
    "parent_df = main_df['parent']\n",
    "reply_df = main_df['comment']\n",
    "parent_df = parent_df.apply(preprocess_sentence)\n",
    "reply_df = reply_df.apply(preprocess_sentence)\n",
    "print('Dataframe cleaned.')\n",
    "\n",
    "'''HYPERPARAMS'''\n",
    "# Maximum sentence length\n",
    "MAX_LENGTH = 50\n",
    "# Maximum number of samples to preprocess\n",
    "MAX_SAMPLES = 200000\n",
    "# For tf.data.Dataset\n",
    "BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
    "BUFFER_SIZE = 20000\n",
    "# For Transformer\n",
    "NUM_LAYERS = 2\n",
    "D_MODEL = 256\n",
    "NUM_HEADS = 8\n",
    "UNITS = 512\n",
    "DROPOUT = 0.1\n",
    "EPOCHS = 40\n",
    "'''HYPERPARAMS'''\n",
    "\n",
    "print(parent_df[20])\n",
    "\n",
    "# Build tokenizer using tfds for both questions and answers\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(parent_df+reply_df, target_vocab_size=2**20)\n",
    "\n",
    "# Define start and end token to indicate the start and end of a sentence\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "# Vocabulary size plus start and end token\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print('Tokenized sample question: {}'.format(tokenizer.texts_to_sequences(parent_df[20])))\n",
    "\n",
    "\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "\n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "        # tokenize sentence\n",
    "        sentence1 = START_TOKEN + tokenizer.texts_to_sequences(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.texts_to_sequences(sentence2) + END_TOKEN\n",
    "        # check tokenized sentence max length\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "\n",
    "    # pad tokenized sentences\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "\n",
    "    return tokenized_inputs, tokenized_outputs\n",
    "\n",
    "\n",
    "tk_parent_df, tk_reply_df = tokenize_and_filter(parent_df, reply_df)\n",
    "print('Vocab size: {}'.format(VOCAB_SIZE))\n",
    "print('Number of samples: {}'.format(len(tk_parent_df)))\n",
    "\n",
    "# remove START_TOKEN from targets\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {'inputs': tk_parent_df,'dec_inputs': tk_reply_df[:, :-1]}, {'outputs': tk_reply_df[:, 1:]},\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "#ATTENTION\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  \"\"\"Calculate the attention weights. \"\"\"\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # scale matmul_qk\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # add the mask to zero out padding tokens\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k)\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.keras.layers.Lambda(lambda inputs: tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth)))(inputs)\n",
    "        return tf.keras.layers.Lambda(lambda inputs: tf.transpose(inputs, perm=[0, 2, 1, 3]))(inputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # linear layers\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # split heads\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # scaled dot-product attention\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "        scaled_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.transpose(\n",
    "            scaled_attention, perm=[0, 2, 1, 3]))(scaled_attention)\n",
    "\n",
    "        # concatenation of heads\n",
    "        concat_attention = tf.keras.layers.Lambda(lambda scaled_attention: tf.reshape(scaled_attention,\n",
    "                                                                                      (batch_size, -1, self.d_model)))(\n",
    "            scaled_attention)\n",
    "\n",
    "        # final linear layer\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "#MASK\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEncoding, self).get_config()\n",
    "        config.update({\n",
    "            'position': self.position,\n",
    "            'd_model': self.d_model,\n",
    "\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        # apply sin to even index in the array\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # apply cos to odd index in the array\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "#ENCODER LAYER\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    attention = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention\")({\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "    add_attention = tf.keras.layers.add([inputs,attention])\n",
    "    attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.add([attention,outputs])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "#ENCODER\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size,d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "      outputs = encoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name=\"encoder_layer_{}\".format(i),\n",
    "      )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "#DECODER LAYER\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "    add_attention = tf.keras.layers.add([attention1,inputs])\n",
    "    attention1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    attention2 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "            'query': attention1,\n",
    "            'key': enc_outputs,\n",
    "            'value': enc_outputs,\n",
    "            'mask': padding_mask\n",
    "        })\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    add_attention = tf.keras.layers.add([attention2,attention1])\n",
    "    attention2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    add_attention = tf.keras.layers.add([outputs,attention2])\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=1e-6)(add_attention)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "#DECODER\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.keras.layers.Lambda(lambda d_model: tf.math.sqrt(tf.cast(d_model, tf.float32)))(d_model)\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='enc_padding_mask')(inputs)\n",
    "    # mask the future tokens for decoder inputs at the 1st attention block\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "    # mask the encoder outputs for the 2nd attention block\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "        create_padding_mask, output_shape=(1, 1, None),\n",
    "        name='dec_padding_mask')(inputs)\n",
    "\n",
    "    enc_outputs = encoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,)(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,)(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "\n",
    "#LOSS\n",
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.constant(d_model, dtype=tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"d_model\": self.d_model, \"warmup_steps\": self.warmup_steps}\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.multiply(tf.math.rsqrt(self.d_model), tf.math.minimum(arg1, arg2))\n",
    "\n",
    "#TRAIN\n",
    "# clear backend\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.8, beta_2=0.9, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "# initialize and compile model within strategy scope\n",
    "with strategy.scope():\n",
    "    model = transformer(\n",
    "      vocab_size=VOCAB_SIZE,\n",
    "      num_layers=NUM_LAYERS,\n",
    "      units=UNITS,\n",
    "      d_model=D_MODEL,\n",
    "      num_heads=NUM_HEADS,\n",
    "      dropout=DROPOUT)\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "model.summary()\n",
    "\n",
    "#TRAINING\n",
    "model.load_weights('/content/drive/MyDrive/reddit_data/secondmodel.h5')\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.texts_to_sequences(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    output = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "        # select the last word from the seq_len dimension\n",
    "        predictions = predictions[:, -1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # return the result if the predicted_id is equal to the end token\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # concatenated the predicted_id to the output which is given to the decoder\n",
    "        # as its input.\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(predicted_sentence))\n",
    "    return predicted_sentence\n",
    "\n",
    "output = predict('Where have you been?')"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS: 1\n",
      "REPLICAS: 1\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16348/2479333020.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mtimeframe\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtimeframes\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 60\u001B[1;33m     \u001B[0mconnection\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msqlite3\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'/content/drive/MyDrive/reddit_data/{}.db'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtimeframe\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     61\u001B[0m     \u001B[0mc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcursor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m     \u001B[0mdf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_sql\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'SELECT * FROM parent_reply'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconnection\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mOperationalError\u001B[0m: unable to open database file"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUQ5GPm7wN33",
    "outputId": "9b9e17f2-d502-468a-98b3-bb0090ee73a1"
   },
   "source": [
    "def reprocess(sentence):\n",
    "    sentence = re.sub(r\" newlinechar\", \"\", sentence)\n",
    "    sentence = re.sub(r\" ,\", \",\", sentence)\n",
    "    sentence = re.sub(r\" \\.\", r\".\", sentence)\n",
    "    sentence = re.sub(r\" !\", \"!\", sentence)\n",
    "    sentence = re.sub(r\" \\?\", r\"?\", sentence)\n",
    "    return sentence\n",
    "\n",
    "def respond(sentence):\n",
    "    prediction = evaluate(sentence)\n",
    "\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Output: {}'.format(reprocess(predicted_sentence)))\n",
    "    return\n",
    "  \n",
    "respond('Hello, are you a bot')\n",
    "respond('Do you like cats')\n",
    "respond('Are you properly trained or do I need to add more databases')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: Hello, are you a bot\n",
      "Output: yeah, i am a dude. i am a pc.\n",
      "Input: Do you like cats\n",
      "Output: you are not alone in a bad person.\n",
      "Input: Are you properly trained or do I need to add more databases\n",
      "Output: yeah, he is a real doctor, not a doctor. he is a mean very strong sense of humour, in the first place.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20FiwuE1QBii",
    "outputId": "4b3d0537-266a-48c1-8339-079273232a1c"
   },
   "source": [
    "respond('do you wank a lot')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: do you wank a lot\n",
      "Output: i do not think so. i am a pc.\n"
     ]
    }
   ]
  }
 ]
}